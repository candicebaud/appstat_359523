---
title: "Report Project 2"
author: "Sciper 359523"
date: "2023-03-06"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message=FALSE, warning=FALSE)
```

```{r working directory and data, echo=FALSE, include=FALSE}
lapply(c("dplyr","chron","ggplot2","tidyr","questionr","survival","forcats","tidyselect",
         "data.table","table1","lubridate", "ggpubr","viridis","finalfit","survminer",
         "ggpubr", "ggthemes", "gridExtra", "rstatix","stringr",
         "wesanderson","kableExtra", "naniar","boot","scales","ggsci", "stringr",
         "Hmisc","DescTools","swimplot", 'stats', 'EnvStats', 'finalfit'), 
       library, character.only=TRUE)

Sys.setlocale("LC_TIME", "English")
load("2_online_shopping.RData")
set.seed(435) #set the seed for reproducible results
```

# 1. Introduction 
In today's data-driven world, businesses are constantly seeking new ways to leverage customer data to make informed decisions and drive growth[1]. One area where data analysis can prove particularly useful is in forecasting customer purchases. By analyzing data on user behavior and purchase history, businesses can gain insights into what products or services customers are likely to buy in the future. Users' reactions are therefore scrutinized to improve sales by better tailoring the ads. 

In this report, we work with a data set providing us several variable such as an outcome variable we want to model, which is a binary indicator of a purchase being made or not. The other variables, that will help build the model are mostly divided between the users' actions recorded by the websites, and the user's personal characteristics.    

In this report, we explore the use of a logistic regression model to predict purchase behavior based on user actions and personal characteristics. Our data set includes several variables, including a binary indicator of whether a purchase was made or not, as well as other user behavior and personal characteristics that serve as predictors. 

The outline of the remainder of this report is as follows. We introduce the data in section 2, describe our model selection procedure in section 3 and present the final model. We analyse our model via the diagnostics in Section 4 and will compare it to other in section 5. In section 6 we conclude on the work done and the potential improvements.


# 2. Data characteristics
The outcome of interest is the fact that a purchase was made or not (variable *purchase*). The data set is composed of several different features that can be decomposed in three main groups :

1. Data coming from the e-commerce website :
+ The number of administrative-type pages that the user visited and the time spent on them (numerical variable)
+ The number of informational-type pages that the user visited and the time spent on them (numerical variable)
+ The number of product-related-type pages that the user visited and the time spent on them (numerical variable)

2. Data derived from the date :
+ The month in which the session took place (categorical variable with 10 levels)
+ An indicator of whether the session took place during the week-end or not (dummy variable)
+ An indicator between [0,1] evaluating the closeness to a special day (numerical variable)

3. Data coming from Google Analytics :
+ The average bounce rate of pages visited (numerical variable)
+ The average exit rate of pages visited (numerical variable)
+ The average page value of pages visited (numerical variable)
+ The operating systems of the user (categorical variable with 8 levels)
+ The web browsers of the users (categorical variable with 13 levels)
+ The geographic region in which the use is located (categorical variable with 9 levels)
+ Where from the user arrived at the site (categorical variable with 20 levels)
+ The type of visitor (categorical variable with 3 levels)



```{r data base work, echo=FALSE, include = FALSE}
df<-Data
  
#below I did not fully rename the variables but instead added some labels so when you look in the database it's nicer, and it's also nicer for plots in general
df <- df %>% mutate(
  #from the e-commerce website
  administrative = as.numeric(Administrative) %>% ff_label("Number of administrative pages visited"),
  administrative_duration = as.numeric(Administrative_Duration)%>% ff_label("Time spent on administrative pages"),
  informational = as.numeric(Informational) %>% ff_label("Number of informational-type pages visited"),
  informational_duration =as.numeric(Informational_Duration) %>% ff_label("Time spent on informational-type pages"),
  product_related = as.numeric(ProductRelated) %>% ff_label("Number of product related type pages visited"),
  product_related_duration = as.numeric(ProductRelated_Duration)%>% ff_label(" Time spent on product related type pages"),
  #derived from a date
  month = as.factor(Month) %>% ff_label("Month of the session"),
  weekend = as.factor(Weekend) %>% ff_label("Indicator of the session during a week-end"),
  special_day = as.numeric(SpecialDay)%>% ff_label("Closeness to a Special day"),
  #from google analytics
  bounce_rates = as.numeric(BounceRates) %>% ff_label("Average bounce rate of pages visited"),
  exit_rates = as.numeric(ExitRates) %>% ff_label('Average exit of pages visited'),
  page_values= as.numeric(PageValues) %>% ff_label('Average page value of pages visited'),
  operating_systems = as.factor(OperatingSystems)%>% ff_label("Operating systems of the user"),
  browser = as.factor(Browser)%>% ff_label("Browser of the user"),
  region = as.factor(Region) %>% ff_label("Geographic region"),
  traffic_type = as.factor(TrafficType)%>% ff_label("Traffic type"),
  visitor_type = as.factor(VisitorType)%>% ff_label("Visitor type"),
  purchase = as.factor(Revenue)%>% ff_label("Indicator of a purchase made or not")
)


df <- df %>% mutate(
  purchase = as.factor(case_when(
    purchase == F ~ 0,
    purchase == T ~ 1
  )))


#final dataframe
df <- df %>% select(purchase,visitor_type,traffic_type,region,browser, operating_systems,page_values, exit_rates, bounce_rates, special_day, weekend, month, product_related_duration, product_related, informational_duration, informational,administrative_duration, administrative)

buy <- 100*(1 - sum(df$purchase==0)/length(df$purchase)) #percentage of purchases
```

The first exploratory data analysis revealed important insights into the correlation structure of the data set. *Figure 1* displays the correlation matrix, with only the most correlated features represented. The variable 'page values' is strongly correlated with purchase, and is also moderately correlated with exit rates and bounce rates, which are themselves highly correlated. Exit rates, in particular, are highly correlated with most of the other variables displayed in the figure, highlighting its importance as a feature that is heavily linked to the other ones.

```{r, fig.align="center", fig.cap="Figure 1 : Correlation matrix"}
panel.corrplot <- function(x, y, digits = 2, prefix = "", cex.cor, ...) {
    usr <- par("usr")
    on.exit(par(usr))
    par(usr = c(0, 1, 0, 1))
    Cor <- abs(cor(x, y)) 
    method = "color"
    txt <- paste0(prefix, format(c(Cor, 0.123456789), digits = digits)[1])
    if(missing(cex.cor)) {
        cex.cor <- 0.4 / strwidth(txt)
    }
    text(0.5, 0.5, txt,
         cex = 1 + cex.cor * Cor)
}
panel.hist <- function(x, ...) {
    usr <- par("usr")
    on.exit(par(usr))
    par(usr = c(usr[1:2], 0, 1.5))
    his <- hist(x, plot = FALSE)
    breaks <- his$breaks
    nB <- length(breaks)
    y <- his$counts
    y <- y/max(y)
    rect(breaks[-nB], 0, breaks[-1], y, col = rgb(0, 1, 1, alpha = 0.5), ...)
}


# Plotting the correlation matrix
suppressWarnings(pairs(df$purchase ~ df$page_values + df$exit_rates + df$bounce_rates + df$product_related_duration + df$product_related + df$administrative + df$visitor_type,
      upper.panel = panel.corrplot,   
      lower.panel = panel.smooth, 
      diag.panel = panel.hist,))
```


Additional descriptive statistics show that purchases in the database represent `r round(buy, digits=1)` % of the data set and are predominantly made by returning visitors. While no particular region stands out, there are noticeable differences in purchasing behavior depending on the browser and traffic type used. Users who make a purchase tend to view more pages and have lower exit and bounce rates compared to those who do not make a purchase. Furthermore, they tend to spend more time on the website and engage more with the content. Surprisingly, weekends and special days do not seem to have a significant effect on sales, although the session month does.

Given these findings, we consider several data set transformations to improve our models. Some categorical variables contain only a few observations for each level, so we use clustering to group levels and increase the sample size. In addition, certain variables are over-dispersed, so we explore log and square root transformations to construct more accurate models.

```{r clustering, echo=FALSE, include = FALSE}
#for traffic_type
traffic_type <- df$traffic_type
hc <- hclust(dist(table(traffic_type)))
plot(hc)

clusters_traffic_type <- cutree(hc, h = 7)
clusters_traffic_type

df<- df %>% mutate(
  grouped_traffic_type = case_when(
    traffic_type == '1'~ '1',
    traffic_type == '2'~ '2',
    traffic_type == '3'~ '3',
    traffic_type == '4'~ '4',
    traffic_type == '5'~ '5',
    traffic_type == '6'~ '6',
    traffic_type == '7'~ '7',
    traffic_type == '8'~ '8',
    traffic_type == '9'~ '7',
    traffic_type == '10'~ '9',
    traffic_type == '11'~ '10',
    traffic_type == '12'~ '11',
    traffic_type == '13'~ '12',
    traffic_type == '14'~ '11',
    traffic_type == '15'~ '11',
    traffic_type == '16'~ '11',
    traffic_type == '17'~ '11',
    traffic_type == '18'~ '11',
    traffic_type == '19'~ '11',
    traffic_type == '20'~ '15',
  )
)


#for browser
browser <- df$browser
hc <- hclust(dist(table(browser)))
plot(hc)

clusters_browser <- cutree(hc, h = 10)
clusters_browser

df<- df %>% mutate(
  grouped_browser = case_when(
    browser == '1'~ '1',
    browser == '2'~ '2',
    browser == '3'~ '3',
    browser == '4'~ '4',
    browser == '5'~ '5',
    browser == '6'~ '6',
    browser == '7'~ '7',
    browser == '8'~ '8',
    browser == '9'~ '7',
    browser == '10'~ '6',
    browser == '11'~ '7',
    browser == '12'~ '7',
    browser == '13'~ '7'
  )
) 


#for operating_systems
op <- df$operating_systems
hc <- hclust(dist(table(op)))
plot(hc)

clusters_op <- cutree(hc, h = 10)
clusters_op

df<- df %>% mutate(
  grouped_operating_systems = case_when(
    operating_systems == '1'~ '1',
    operating_systems == '2'~ '2',
    operating_systems == '3'~ '3',
    operating_systems == '4'~ '4',
    operating_systems == '5'~ '1',
    operating_systems == '6'~ '1',
    operating_systems == '7'~ '1',
    operating_systems == '8'~ '5'
  )
) 

df <- df %>%mutate(grouped_browser = as.factor(grouped_browser), grouped_traffic_type = as.factor(grouped_traffic_type), grouped_operating_systems = as.factor(grouped_operating_systems))

df <- df%>%select(purchase,visitor_type,grouped_traffic_type,region,grouped_browser, grouped_operating_systems,page_values, exit_rates, bounce_rates, special_day, weekend, month, product_related_duration, product_related, informational_duration, informational,administrative_duration, administrative)

```


```{r adding variables, echo=FALSE, include = FALSE}
#log transformations
df <- df %>% mutate(
  log_exit = log(exit_rates +1 ),
  log_bounce = log(bounce_rates +1),
  log_administrative = log(administrative +1),
  log_admin_duration = log(administrative_duration +1),
  log_prod_related_duration = log(product_related_duration +1),
  log_prod_related = log(product_related +1),
  log_inf_duration = log(informational_duration+1),
  log_inf = log(informational_duration +1),
  log_page_values = log(page_values +1)
)

#sqrt transformation
df <- df %>% mutate(
  sqrt_exit = sqrt(exit_rates),
  sqrt_bounce = sqrt(bounce_rates),
  sqrt_administrative = sqrt(administrative),
  sqrt_time_admin = sqrt(administrative_duration),
  sqrt_time_duration = sqrt(product_related_duration)
)

#interaction terms
df <- df %>% mutate(
  inter_product = product_related * product_related_duration,
  inter_informational = informational * informational_duration,
  inter_admin = administrative * administrative_duration
)
```



# 3. Model selection 

```{r model building, echo=FALSE, include = FALSE}
m0_0 <- glm(purchase ~visitor_type + grouped_traffic_type + region + grouped_browser + grouped_operating_systems + page_values + exit_rates + bounce_rates + special_day + weekend + month + product_related_duration + product_related + informational_duration + informational + administrative_duration + administrative, family = 'binomial', data = df) #without the new included variables 

m0 <- glm(purchase ~., family = 'binomial', data = df) #with all the variables, including the new ones 

#to compare the first two models
summary(m0_0)
summary(m0)

m0_0$aic
m0$aic

m0_0$deviance
m0$deviance #adding the variables improved both the AIC and the deviance

#now I build a better model with the function stepAIC which will select the best model in terms of AIC
library('MASS')
#model <- stepAIC(m0,direction = c("both", "backward", "forward"), trace = FALSE)

model <- glm(purchase ~ visitor_type + grouped_traffic_type + 
    page_values + exit_rates + month + product_related_duration + 
    administrative + sqrt_exit + sqrt_bounce + sqrt_administrative + 
    sqrt_time_duration, family = "binomial", data =df)#this is the model given by stepAIC but as it's very long, I include it here directly
summary(model)

#to see if the variables selected are statistically significant
Anova(model, type = 'II', test ='LR') #all of them are significant
Anova(m0, type = 'II', test ='LR') #most of them are not significant

#trying to add some variables since m0 seems better
improved_model <- glm(purchase ~ visitor_type + grouped_traffic_type + 
    page_values + exit_rates + bounce_rates + month + product_related_duration + 
    administrative + log_admin_duration +log_prod_related_duration + log_page_values + sqrt_exit + sqrt_bounce + sqrt_administrative + 
    sqrt_time_duration, family = "binomial", data =df)
summary(improved_model)
Anova(improved_model, type = 'II', test ='LR') #sqrt_administrative is not significant

#by removing non significative variables one by one, I obtain the final model below
improved_model_2 <- glm(purchase ~ visitor_type + grouped_traffic_type + 
    page_values + exit_rates + bounce_rates + month + product_related_duration + 
    administrative +log_prod_related_duration + log_page_values + sqrt_exit + sqrt_bounce  + 
    sqrt_time_duration, family = "binomial", data =df)
summary(improved_model_2)
anova_table <- Anova(improved_model_2, type = 'II', test ='LR') #with this one all the variables are significative under the 5%

#odds ratio and confidence intervals
table_results_OR <- exp(cbind(OR = coef(improved_model_2), confint(improved_model_2)))

```

To build the model, I start by computing the full model without the added variables and compare it to the one with added variables to assess their relevance. The model with added variables has a smaller AIC (`r round(m0$aic, digits =0)` compared to `r round(m0_0$aic, digits=0)`) as well as a smaller deviance (`r round(m0$deviance, digits=0)` compared to `r round(m0_0$deviance, digits=0)`) which gives confidence in those new variables.

I then used the stepAIC function of the MASS package to perform AIC-based model selection. This created a basis model that I improved by examining the significance of the variables with the Anova command. This allowed me to build an improved model by adding variables one by one and simultaneously testing their significance.

The final model includes the variables: the visitor type, the traffic type, the page values and its log, the exit rates and their square root, the bounce rates and their square root, the session month, the time spent looking at product-related information and its log and square root, the number of administrative pages looked at. All of these variables are significant at the 5% level, and most of them are even significant below the 0.1% level, which provides strong evidence for the model's reliability. The AIC of our final model is  `r round(improved_model_2$aic, digits=0)` and the deviance is `r round(improved_model$deviance, digits=0)`. 


```{r, echo=FALSE, include = FALSE}
#Summary of the final model
summary(improved_model_2)
```


```{r, echo=FALSE, include = FALSE}
#Odds ratio and confidence intervals
table_results_OR
```

The table below reports the three most practically significant variables (in terms of odds ratio) and their confidence interval.

|                    | OR       | 2.5%     | 97.5%       | 
|--------------------|----------|----------|-------------|
| bounce rates       | 9.16e+08 | 1.05e+01 | 2.58e+16    | 
| sqrt(exit rates)   | 5.23e+03 | 2.68e+01 | 1.15e+06    | 
| log(page values)   | 3.73     | 3.47     | 4.01        | 

They are statistically significant (with the Type II test, and because 0 doesn't belong to the confidence interval). They are also practically significant because their odds ratio are greater than 1. Indeed, half of variables have odds ratio greater than 1 in the overall table and the rest mostly belongs to [0.2, 0.9]. Only the untransformed exit rate variable doesn't seem practically significant even though it is statistically significant, since the odds ratio associated is of order $10^{-18}$. The fact that the model also contains its square root captures most of the effect of the variable.

```{r, echo=FALSE, include= FALSE}
#Anova of the final model
anova_table
```

# 4. Model diagnostics
*Figure 2* shows the diagnostics of the model. On the upper left panel, one can see the characteristic two-bands pattern associated with logistic regressions. There are several visible outliers that appear to be influential on the model as identified in the *scale-location* and *residuals vs leverage panels*. The normal QQplot for the logistic regression also identifies some outliers in the upper tail.

```{r, fig.align="center", fig.cap="Figure 2 : Diagnostics"}
par(mfrow=c(2,2))
plot(improved_model_2)
```

# 5. Model comparison 
In this section, I compare the final model to the initial models to see to what extent the work on the base helped us build a better model. The red curve represents the initial model without the added variables (ie the log and sqrt transormations). The black curve represents the one built with all the variables of the data set. Finally, the blue curve represents the model resulting from the *stepAIC* function, and the green curve represents the final model. The AUC that is displayed is the one calculated in the sample for the final model. 

The green one appears to really improve the model compared to the one built only by the *stepAIC* function. Transforming variables also clearly improves the ROC curve and thus the model.

```{r, fig.align="center", fig.cap="Figure 3 : ROC curves : initial model without added variables (red), model with all the variables (black), model with only variable selection (blue), final model (green)"}
library(pROC)
invisible(plot(roc(df$purchase,
                   fitted(m0_0)),
               col = "red", 
               print.auc = F,
               main = "ROC curves"))
invisible(plot(roc(df$purchase,
                   fitted(model)),
               print.auc = F, 
               col = "blue", 
               add = T))
invisible(plot(roc(df$purchase,
                   fitted(m0)),
               print.auc = F, 
               col = "black", 
               add = T))
invisible(plot(roc(df$purchase,
                   fitted(improved_model_2)),
               print.auc = T, 
               col = "green", 
               add = T))

legend("bottomright",legend=c("Initial model without added variables", "Model with only variable selection", "Model with all the variables", "Final model" ),
       col=c("red", "blue", "black", "green"), lty=1, cex=0.8)
```

```{r, echo=FALSE}
#code provided
library(pROC)
AUC_eval <- function(gmodel,Data){
set.seed(517)
Folds <- matrix(sample(1:dim(Data)[1]), ncol=5)
AUC <- rep(0,5)
for(k in 1:5){
train <- Data[-Folds[,k],]
test <- Data[Folds[,k],]
my_gm <- glm(gmodel$formula, family="binomial", data=train)
test_pred <- predict(my_gm, newdata = test, type="response")
AUC[k] <- auc(test$purchase,test_pred)
}
return(mean(AUC))
}


auc_m0 <- AUC_eval(m0,df)
auc_m0_0 <- AUC_eval(m0_0, df)
auc_model <- AUC_eval(model,df)
auc_improved_model <- AUC_eval(improved_model,df)
auc_improved_model_2 <- AUC_eval(improved_model_2,df)

```

By denoting by model0 the basis model without added variables, model1 the model with all the added variables, model2 the one obtained with *stepAIC* and model3 the final model, the performance results are as follows :  

|                    | model0 | model1 | model2 | model3 |
|--------------------|--------|--------|--------|--------|
| AIC                | 7167   | 6078   | 7094   | 6042   |
| AUC                | 0.88   | 0.911  | 0.889  | 0.913  |
| Residuals deviance | 7059   | 5938   | 7030   | 5975   |


# 6. Discussion
In conclusion, the logistic regression model developed in this project is satisfactory with all variables being significant at the 5% level. The model also outperformed other models based on AIC, AUC, and residuals deviance, which instills confidence in its accuracy. However, the presence of influential outliers in the dataset warrants further investigation, and their removal could potentially lead to an even more improved model. 

It's worth noting that the clustering of variable levels was necessary to avoid having levels with insufficient data. However, collecting more data to include all levels could potentially improve the accuracy of the model even further.

# References
[1] https://cmr.berkeley.edu/2018/04/facebook-ads/