---
title: "Report project number 1"
author: "Sciper 359523"
date: "2023-02-21"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message=FALSE, warning=FALSE)
```

```{r working directory and data, echo=FALSE, include=FALSE}
lapply(c("dplyr","chron","ggplot2","tidyr","questionr","survival","forcats","tidyselect",
         "data.table","table1","lubridate", "ggpubr","viridis","finalfit","survminer",
         "ggpubr", "ggthemes", "gridExtra", "rstatix","stringr",
         "wesanderson","kableExtra", "naniar","boot","scales","ggsci", "stringr",
         "Hmisc","DescTools","swimplot", 'stats', 'EnvStats'), 
       library, character.only=TRUE)

Sys.setlocale("LC_TIME", "English")
df <- read.csv("1_snow_particles.csv")
set.seed(435) #set the seed for reproducible results
```


## 1. Introduction
Snow particle size is a critical feature used to model and understand the dynamics of avalanches, particularly large powder-snow avalanches that behave like fast-moving clouds down mountainsides [1]. The sedimentation velocity, which is highly influenced by particle size, plays a significant role in the dynamic of turbulent dilute suspension avalanches [1]. Accurate modeling of sedimentation velocity is crucial, as errors in size estimation can lead to 10x greater errors in velocity estimation.

In this report, I analyze a dataset of snow particle sizes obtained from a PhD student at the Laboratory of Cryospheric Sciences at EPFL. The dataset includes the number of particles in each size category, which is the most important feature for the analysis. I begin by performing exploratory data analysis to gain insights into the dataset and create a preliminary model. I then enhance this model using local optimization. Finally, I assess the model's quality by performing parametric bootstrap analysis.


## 2. Data Characteristics
The outcome of interest is the variable *retained* which is the fraction of particles belonging to each diameter bin. The other available variables are 

+ The startpoint of the bin,
+ The endpoint of the bin,
+ The number of particles detected,

This study aims to simulate snowflake diameters that accurately reflect the data, in order to investigate the aeolian transport of snow.

*Figure 1* displays the percentage of particles in each bin relative to the bin length, with the distribution plotted according to the bin startpoint. The figure suggests that the data can be described by a mixture of log-normal distributions, with two visible distinct peaks. The bin interval lengths are different so I divide my initial proportions by the interval length in order to accurately represent the distribution.


```{r, fig.align="center", fig.cap="Figure 1: Percentage of particles in each bin"}
df$binsize <- df$endpoint - df$startpoint #size of the intervals
plot(df$startpoint,(df$retained..../(100*df$binsize)), main ='Percentage of particles in each bin', xlab = 'Bin startpoint', ylab ='Proportion of particles(%)', type ='l', col='red')
```



## 3. Model fitting
After identifying that the data may be distributed as a mixture of two log-normal distributions, the next step is to evaluate the parameters for the density function:

$f(x) = p \frac{1}{x \sigma_1 \sqrt{2\pi}} e^{-\frac{(\ln x - \mu_1)^2}{2\sigma_1^2}} + (1-p) \frac{1}{x \sigma_2 \sqrt{2\pi}} e^{-\frac{(\ln x - \mu_2)^2}{2\sigma_2^2}}$

To evaluate the five parameters $p$, $\mu_1$, $\sigma_1$, $\mu_2$, and $\sigma_2$, I will use the Expectation-Maximization (EM) algorithm, which is a widely used iterative method for estimating the parameters of statistical models with latent variables.

In the first step of the EM algorithm, I will initialize the parameters and estimate the posterior distribution of the latent variables. In the second step, I will update the parameters using the expected values of the latent variables computed in the first step. I will repeat these two steps until convergence.

After obtaining an initial estimate of the parameters using the EM algorithm, I will explore other methods to improve the estimation, such as local search and Bayesian approaches.

### EM algorithm 
I implement hereafter an EM-algorithm taking as initialization parameters 

$\mu_1 = 0.25$, $\sigma_1 = 0.75$, $\mu_2 = 0.25$, $\sigma_2 = 0.5$, $p = 0.5$

$\epsilon = 0.001$ will constitute the threshold such that I stop the algorithm when the difference in the log-likelihoods is under $\epsilon$.

The EM algorithm components are obtained with the following reasoning. 
Let $X_1,\ldots,X_N$ be i.i.d. from
$$f_{mixture}(x) = p \varphi_{lognormal,\mu_1,\sigma_1^2}(x) + (1-p) \varphi_{lognormal,\mu_2,\sigma_2^2}(x)$$
where $$\varphi_{lognormal,\mu_1,\sigma_1^2}(x)$$ corresponds to the density of the lognormal distribution with parameters $\mu_1$ and $\sigma_1^2$. 
The log likelihood doesn't have a good form so I draw $Zn = \mathbb{I}_{\left[X_n \text{ drawn from }\varphi_{\mu_2,\sigma_2^2}\right]} \sim \mathrm{Bern}(p)$ iid and independent of the $X_i$. One can rewrite the likelihood as \[
\ell_{comp}(\theta) = \sum_{n=1}^N \left\{ (1-Z_n)\left[ \log \varphi_{lognormal, \mu_1,\sigma_1^2}(X_n) + \log(p) \right] + Z_n\left[ \log \varphi_{lognormal, \mu_2,\sigma_2^2}(X_n) + \log(1-p) \right] \right\}
\]

One can then define \[
E_{\theta^{(l-1)}}\big[\ell_{comp}(\theta) \big| X_1,\ldots,X_n\big] = P(Z_n=1|X_n) = \frac{P(X_n|Z_n=1) P(Z_n=1)}{P(X_n)} =: \gamma_n^{(l-1)}
\]

The M-step allows us to update the parameters in the following way 
\[
\begin{split}
p^{(l)} &= N^{-1} \gamma, \quad \text{where} \quad \gamma = \sum_{n=1}^N \gamma^{(l-1)}_n \\
\mu_1^{(l)} &= \gamma^{-1} \sum_{n=1}^N \big(\gamma^{(l-1)}_n\big) log(X_N) \\
(\sigma_1^2)^{(l)} &= \gamma^{-1} \sum_{n=1}^N \big(\gamma^{(l-1)}_n\big) \big(log(X_n) - \mu_1^{(l)} \big)^2 \\
\mu_2^{(l)} &= (N-\gamma)^{-1} \sum_{n=1}^N (1-\gamma^{(l-1)}_n) log(X_N) \\
(\sigma_2^2)^{(l)} &= (N-\gamma)^{-1} \sum_{n=1}^N (1-\gamma^{(l-1)}_n) \big(log(X_n) - \mu_2^{(l)} \big)^2 \\
\end{split}
\]

```{r densities, echo=FALSE}
#lognormal generates the lognormal density function
lognormal <- function(x, mu1, sigma1){
  if (x>0){
    return((1/x)*dnorm(log(x), mean=mu1, sd=sigma1))}
  else{
    return(0)
  }
}

#bilognorm generates the mixture of two lognormal density function
bilognorm <- function(x, mu1, mu2, sigma1, sigma2, p) {
    return(p*lognormal(x,mu1,sigma1)+(1-p)*lognormal(x,mu2,sigma2))
}
```

Before doing my EM-algorithm, I do some jittering. I simulate points uniformly in the different diameters intervals in a proportion according to the percentage of particles in each bin. I simulate 705044 points which corresponds to the total number of particles detected.

```{r EM algo jittering, echo=FALSE}
#simulate 705044 points 
n_points <- trunc(df$particles.detected[1]) #total of points I will take
X <- c()
int_size <- numeric(length(df$retained....)) #number of points in each interval
for (i in 1:52){
  int_size[i]<- df$retained....[i]*n_points/100
}
for (i in 1:52){
  r <- runif(int_size[i], min=df$startpoint[i], max = df$endpoint[i])
  X <- append(X, r)
}


#Initialisation of the EM algorithm 
p_init <- 0.5
mu1_init <- 0.25
mu2_init <- 0.75
sigma1_init <- 0.25
sigma2_init <- 0.5

#function to estimate the parameters
estimate <- function(X, p_init, mu1_init, mu2_init, sigma1_init, sigma2_init){
Y <- numeric(length = length(X))
for (i in 1:length(X)){
  if (X[i]>0){
    Y[i] <- log(X[i])}}
N = length(X)

#criterion of convergence : epsilon
epsilon = 0.001
k=0
err = 20000 #error to which we will compare epsilon
old_likelihood <- 0 
new_likelihood <- 0 
p_new <- numeric(length=N)

#I put a limit of k interations to have a faster algorithm (especially useful in the Bootstrap algorithm)
while (err>epsilon & k<20){
  
#log likelihood computation
for (i in 1:length(X)){
  old_likelihood <- old_likelihood + bilognorm(X[i], mu1_init, mu2_init, sigma1_init, sigma2_init, p_init)} 
old_likelihood <- log(old_likelihood)

#new parameters
f_theta <- numeric(length(X))
gamma<- numeric(length(X))
for(i in 1:length(X)){
f_theta[i] <- bilognorm(X[i], mu1_init, mu2_init, sigma1_init, sigma2_init, p_init)
if(f_theta[i]>0){
gamma[i] <- p_init*lognormal(X[i], mu1_init, sigma1_init)/f_theta[i]}else{gamma[i]<-1}}

s <- sum(gamma)
p_new <- s/N
mu_1_new <- sum(Y*gamma)/s
mu_2_new <- sum(Y*(1-gamma))/(N-s)
s1 <- sum(gamma*(Y - mu1_init)**2)
s2 <- sum((1-gamma)*(Y - mu2_init)**2)
sigma1_new <- sqrt(s1/s)
sigma2_new <- sqrt(s2/(N-s))

#new log likelihood computation
for (i in 1:length(X)){
  new_likelihood <- new_likelihood + bilognorm(X[i], mu_1_new, mu_2_new, sigma1_new, sigma2_new, p_new)}
new_likelihood <- log(new_likelihood)

#k represents the number of iterations before convergence
k = k+1

#err is the error 
err = abs(new_likelihood- old_likelihood)

#new parameters
mu1_init <- mu_1_new
mu2_init <- mu_2_new
sigma1_init <- sigma1_new
sigma2_init <- sigma2_new
p_init <- p_new

#I put the likelihood at 0 to be able to compute them again 
old_likelihood <- 0 
new_likelihood <- 0 


}
return(c(mu1_init, mu2_init, sigma1_init, sigma2_init, p_init))}

param <- estimate(X = X, p_init = p_init, mu1_init = mu1_init, mu2_init = mu2_init, sigma1_init = sigma1_init, sigma2_init = sigma2_init )

```

The parameters that I obtain are the following 
$\mu_1 = - 0.43$
$\mu_2 = - 1.71$
$\sigma_1 = 0.26$
$\sigma_2 = 0.96$
$p = 0.54$

With the parameters I obtained, I generate *Figure2* which shows the fitted model and our data. The red curve represents the data we aim at modeling and the black one represents our model estimation. I will keep this convention throughout the report.

```{r, fig.align="center", fig.cap="Figure 2: Fitted distribution"}
#the new parameters(estimated with EM)
mu1_init <- param[1]
mu2_init <- param[2]
sigma1_init <- param[3]
sigma2_init <- param[4]
p_init <- param[5]

#to plot
to_plot_x <- seq(0, 1.5, by=0.01)
to_plot_x <- append(to_plot_x, seq(1.5, 3, by=0.5))
to_plot_y <- numeric(length(to_plot_x))
for (i in 1:length(to_plot_x)){
 to_plot_y[i] <- bilognorm(to_plot_x[i], mu1_init, mu2_init, sigma1_init, sigma2_init, p_init)}


plot(df$startpoint,(df$retained..../(100*df$binsize)), col = 'red', type='l', main ='Fitted distribution and real data', xlab ='Bin startpoint', ylab='Proportion of particles (%)')
lines(to_plot_x, to_plot_y, type='l')


#values I obtained and that I want to optimize 
mu1 <- mu1_init
mu2 <- mu2_init
sigma1 <- sigma1_init
sigma2 <- sigma2_init
p <- p_init
```

Although the density I obtained approximately fits the data, it is not fully satisfying because the left peak is under-estimated. To improve the fit, we consider using local optimization techniques.

### Stability of the EM algorithm solution and local modifications
To evaluate the stability of my EM-algorithm solutions, I want to see how our estimated density behaves when I slightly modify one or several parameters. I define an $\epsilon = 0.05$ that I add or subtract to my parameters to see to what extent this changes the model. The different setups are the following 

| Setup |  mean 1       |     mean 2  |    standard deviation 1| standard deviation 2     |    probability  |
|---------|--------------|------------|---------------|---------------|----------|
| Setup 1 | $\mu_1$       |     $\mu_2$ |    $\sigma_1$ | $\sigma_2$     |    $p$  |
| Setup 2 | $\mu_1 + \epsilon$ | $\mu_2+\epsilon$ | $\sigma_1+\epsilon$ | $\sigma_2+\epsilon$ | $p+2\epsilon$ |
| Setup 3 | $\mu_1 - \epsilon$ | $\mu_2-\epsilon$ | $\sigma_1-\epsilon$ | $\sigma_2-\epsilon$ | $p+2\epsilon$ |
| Setup 4 | $\mu_1 - \epsilon$ | $\mu_2+\epsilon$ | $\sigma_1+\epsilon$ | $\sigma_2+\epsilon$ | $p+2\epsilon$ |
| Setup 5 | $\mu_1 + \epsilon$ | $\mu_2-\epsilon$ | $\sigma_1$     | $\sigma_2$     | $p+2\epsilon$ |
| Setup 6 | $\mu_1 - \epsilon$ | $\mu_2+\epsilon$ | $\sigma_1$     | $\sigma_2$     |$p+\epsilon$ |

where $\mu_1$, $\sigma_1$, $\mu_2$, $\sigma_2$ and $p$ are the parameters estimated with the EM algorithm. In other words, setup 1 uses the parameters from the EM algorithm. The other plots are then to be compared to this reference setup.

The results are given by *Figure3*. The obtained graphs do not seem to have improved compared to the EM-algorithm. Thus, I am going to optimize the likelihood in order to find better parameters.


```{r, fig.align="center", fig.cap="Figure 3: Local modification of parameters"}
epsilon = 0.05
par(mfrow = c(2,3))

#graph1
to_plot_x <- seq(0, 1.5, by=0.01)
to_plot_x <- append(to_plot_x, seq(1.5, 3, by=0.5))
to_plot_y <- numeric(length(to_plot_x))
for (i in 1:length(to_plot_x)){
 to_plot_y[i] <- bilognorm(to_plot_x[i], mu1_init, mu2_init, sigma1_init, sigma2_init, p_init)}

plot(df$startpoint,(df$retained..../(100*df$binsize)), col = 'red', type='l', main ='Distribution with setup 1', xlab ='Bin startpoint', ylab='Proportion of particles (%)')
lines(to_plot_x, to_plot_y, type='l')

#graph2
to_plot_x <- seq(0, 1.5, by=0.01)
to_plot_x <- append(to_plot_x, seq(1.5, 3, by=0.5))
to_plot_y <- numeric(length(to_plot_x))
for (i in 1:length(to_plot_x)){
 to_plot_y[i] <- bilognorm(to_plot_x[i], mu1_init+epsilon, mu2_init+epsilon, sigma1_init+epsilon, sigma2_init+epsilon, p_init+2*epsilon)}

plot(df$startpoint,(df$retained..../(100*df$binsize)), col = 'red', type='l', main ='Distribution with setup 2', xlab ='Bin startpoint', ylab='Proportion of particles (%)')
lines(to_plot_x, to_plot_y, type='l')

#graph3
to_plot_x <- seq(0, 1.5, by=0.01)
to_plot_x <- append(to_plot_x, seq(1.5, 3, by=0.5))
to_plot_y <- numeric(length(to_plot_x))
for (i in 1:length(to_plot_x)){
 to_plot_y[i] <- bilognorm(to_plot_x[i], mu1_init-epsilon, mu2_init-epsilon, sigma1_init-epsilon, sigma2_init-epsilon, p_init+2*epsilon)}

plot(df$startpoint,(df$retained..../(100*df$binsize)), col = 'red', type='l', main ='Distribution with setup 3', xlab ='Bin startpoint', ylab='Proportion of particles (%)')
lines(to_plot_x, to_plot_y, type='l')

#graph4
to_plot_x <- seq(0, 1.5, by=0.01)
to_plot_x <- append(to_plot_x, seq(1.5, 3, by=0.5))
to_plot_y <- numeric(length(to_plot_x))
for (i in 1:length(to_plot_x)){
 to_plot_y[i] <- bilognorm(to_plot_x[i],mu1_init-epsilon, mu2_init+epsilon, sigma1_init+epsilon, sigma2_init+epsilon, p_init+2*epsilon)}

plot(df$startpoint,(df$retained..../(100*df$binsize)), col = 'red', type='l', main ='Distribution with setup 4', xlab ='Bin startpoint', ylab='Proportion of particles (%)')
lines(to_plot_x, to_plot_y, type='l')

#graph5
to_plot_x <- seq(0, 1.5, by=0.01)
to_plot_x <- append(to_plot_x, seq(1.5, 3, by=0.5))
to_plot_y <- numeric(length(to_plot_x))
for (i in 1:length(to_plot_x)){
 to_plot_y[i] <- bilognorm(to_plot_x[i],mu1_init+epsilon, mu2_init-epsilon, sigma1_init, sigma2_init, p_init+2*epsilon)}

plot(df$startpoint,(df$retained..../(100*df$binsize)), col = 'red', type='l', main ='Distribution with setup 5', xlab ='Bin startpoint', ylab='Proportion of particles (%)')
lines(to_plot_x, to_plot_y, type='l')

#graph6 
to_plot_x <- seq(0, 1.5, by=0.01)
to_plot_x <- append(to_plot_x, seq(1.5, 3, by=0.5))
to_plot_y <- numeric(length(to_plot_x))
for (i in 1:length(to_plot_x)){
 to_plot_y[i] <- bilognorm(to_plot_x[i],mu1_init-epsilon, mu2_init+epsilon, sigma1_init, sigma2_init, p_init+epsilon)}

plot(df$startpoint,(df$retained..../(100*df$binsize)), col = 'red', type='l', main ='Distribution with setup 6', xlab ='Bin startpoint', ylab='Proportion of particles (%)')
lines(to_plot_x, to_plot_y, type='l')

```

### Local optimization
Local optimization seeks to find the values of the parameters that minimize a given objective function, by searching the parameter space around an initial point. I use the r function *optim* to estimate my parameters starting from the ones estimated with the EM-algorithm.

```{r local optimization, echo=FALSE}
mixnorm <- function(param, X){
  return(p=param[5]*plnorm(X, param[1], param[3])+(1-param[5])*plnorm(X, param[2], param[4]))
} #cdf mixture

#log likelihood to optimize
log_likelihood <- function(param){
  res <- 0 
  for(i in 1:52){
    res <- res + int_size[i]*log(mixnorm(param, df$endpoint[i]) - mixnorm(param, df$startpoint[i]))}
  return(-res)
}

#old parameters 
param <- c(mu1_init, mu2_init, sigma1_init, sigma2_init, p_init)

#new optimized parameters
opt_param <- optim(par = param, fn = log_likelihood, gr=NULL)
```

With local optimization, the new obtained parameters are 
$mu_1 = -0.45$ $\sigma_1 = 0.30$, $mu_2=-2.01$, $sigma_2 = 0.60$, $p=0.65$ 

*Figure4* represents in red the data distribution, in black the one with parameters estimated with the EM-algorithm and in blue the one with the optimized parameters. The blue curve fits the data better than the black one which indicates the optimization is successful.  

```{r,  fig.align="center", fig.cap="Figure 4: Comparison of the distributions with the different estimated parameters"}
#to plot
to_plot_x <- seq(0, 1.5, by=0.01)
to_plot_x <- append(to_plot_x, seq(1.5, 3, by=0.5))
to_plot_y <- numeric(length(to_plot_x))
to_plot_z <- numeric(length(to_plot_x))
for (i in 1:length(to_plot_x)){
 to_plot_y[i] <- bilognorm(to_plot_x[i], mu1_init, mu2_init, sigma1_init, sigma2_init, p_init)
 to_plot_z[i]<- bilognorm(to_plot_x[i], opt_param$par[1], opt_param$par[2], opt_param$par[3], opt_param$par[4], opt_param$par[5])}

plot(df$startpoint,(df$retained..../(100*df$binsize)), col = 'red', type='l', main ='Comparison of the models', xlab ='Bin startpoint', ylab='Proportion of particles (%)')
lines(to_plot_x, to_plot_y, type='l')
lines(to_plot_x, to_plot_z, type = 'l', col='blue')
```

### Parametric bootstrap
In this section, a parametric bootstrap algorithm [2] is implemented to assess if the data truly comes from a mixture of lognormal distributions. The purpose of the algorithm is to sample from the fitted distribution using the estimated parameters, and then calculate the Kolmogorov-Smirnov statistic, denoted by $T$. To do so, I sample from the fitted distribution with the estimated parameters. I use the Kolmogorov-Smirnov statistic $T = \sup_x \Big| \widehat{F}_N(x) - F_\widehat{\lambda}(x) \Big|$, where one assumes that under $H_0$ the estimated parameters $\widehat{\lambda}$ are consistent to the real parameters ${\lambda}_0$.

The null hypothesis I want to test is thus 

$H_0:F \in \mathcal{F} = \{ F_\lambda \mid \lambda \in \Lambda \}$ against $H_1: F \notin \mathcal{F}$ if $\mathcal{F}=\{F_0 \}$

To execute the parametric bootstrap algorithm, the procedure outlined in [2] is followed. Resamples are generated, and the parameters are estimated on these new resamples. The empirical distribution function is then calculated, allowing for the calculation of the Kolmogorov-Smirnov statistic. The p-value of the test can then be computed using the T statistic.


```{r parametric bootstrap 2, echo=FALSE}
#compute the T stat
T_stat <- function(xn, mu1, mu2, sigma1, sigma2,p){
  F_N <- ecdf(xn) #the ecdf of our sample
  n = length(xn) #size of the sample
  #evaluate_diff <- runif(n, min(xn), max(xn))
  evaluate_diff <- xn
  sample <- rep(0,n)
  for (i in 1:n){
    sample[i] <- bilognorm(evaluate_diff[i], mu1, mu2, sigma1, sigma2, p)
  }
  F_star <- ecdf(sample)
  difabs <- abs(F_N(evaluate_diff) - F_star(evaluate_diff) )
  return(max(difabs))
}



#parameters I estimated before
mu1_boot <- opt_param$par[1]
mu2_boot <- opt_param$par[2]
sigma1_boot <- opt_param$par[3]
sigma2_boot <- opt_param$par[4]
p_boot <- opt_param$par[5]
T_0 <- T_stat(X, mu1_boot, mu2_boot, sigma1_boot, sigma2_boot, p_boot) #ok 

#Parametric Bootstrap
Bootstrap <- function(xn, T_0, mu1, mu2, sigma1, sigma2, p, B){
  n = length(xn)
  s <- 0
  #Bootstrap
  T_boot <- rep(NA, B)
  x_resamples <- matrix(NA, nrow = B, ncol = n) #to put our resamples
  for (i in 1:B) {
    x_resamples[i,] <- sample(xn, replace = TRUE)
  }
  for(i in 1:B){
    param <- estimate(x_resamples[i,], p, mu1, mu2, sigma1, sigma2)
    mu1_estim <- param[1]
    mu2_estim <- param[2]
    sigma1_estim <- param[3]
    sigma2_estim <- param[4]
    p_estim <- param[5]
    T_boot[i] <- T_stat(x_resamples[i,], mu1_estim, mu2_estim, sigma1_estim, sigma2_estim, p_estim) 
  }
  for (i in 1:length(T_boot)){
    if (T_boot[i]>T_0){
      s <- s+1
    }
  }
  return((1/(B+1))*(s))
}

#p value
b <- Bootstrap(X, T_0, mu1, mu2, sigma1, sigma2, p, 10)
```

The obtained p-value is of `r b` so I don't reject the null hypothesis. In other words, one concludes that the hypothesis that our data comes from a mixture of two lognormal distributions is not rejected.

## 4. Discussion and conclusion
Throughout this project, I have focused on the mixture of two lognormal distributions as it fits the data well, and my parametric bootstrap algorithm did not reject it. Estimating the parameters of this distribution is challenging, as there is no closed-form formula for the maximum likelihood estimate. I have used a combination of the EM algorithm and local optimization to estimate the parameters, which resulted in a slightly improved fit of the model. The initial parameters for the EM algorithm were chosen based on descriptive statistics and provided a satisfactory starting point for the optimization.The most important limit of this work is in terms of computational cost. 

To conclude, this work enables now to simulate diameters accurately using the mixture of two lognormal distributions following this equation  

$f(x) = p \frac{1}{x \sigma_1 \sqrt{2\pi}} e^{-\frac{(\ln x - \mu_1)^2}{2\sigma_1^2}} + (1-p) \frac{1}{x \sigma_2 \sqrt{2\pi}} e^{-\frac{(\ln x - \mu_2)^2}{2\sigma_2^2}}$

with the parameters 

$\mu_1 = -0.45$ $\sigma_1 = 0.30$, $\mu_2=-2.01$, $\sigma_2 = 0.60$, $p=0.65$ 


## 5. References
[1] https://hal.inria.fr/inria-00405714/document \\

[2] https://github.com/TMasak/StatComp/blob/master/Notes/10_Bootstrap.Rmd, section 'Parametric bootstrap and goodness of fit' \\

[3]https://github.com/TMasak/StatComp/blob/master/Slides/13_BayesComp.pdf 
