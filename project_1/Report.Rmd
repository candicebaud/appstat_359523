---
title: "Report project number 1"
author: "Sciper 359523"
date: "2023-02-21"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message=FALSE, warning=FALSE)
```

```{r working directory and data, echo=FALSE, include=FALSE}
lapply(c("dplyr","chron","ggplot2","tidyr","questionr","survival","forcats","tidyselect",
         "data.table","table1","lubridate", "ggpubr","viridis","finalfit","survminer",
         "ggpubr", "ggthemes", "gridExtra", "rstatix","stringr",
         "wesanderson","kableExtra", "naniar","boot","scales","ggsci", "stringr",
         "Hmisc","DescTools","swimplot", 'stats', 'EnvStats'), 
       library, character.only=TRUE)

Sys.setlocale("LC_TIME", "English")
df <- read.csv("1_snow_particles.csv")
set.seed(435)
```


## 1. Introduction
Snow particle size is a critical feature used to model and understand the dynamics of avalanches, particularly large powder-snow avalanches that behave like fast-moving clouds down mountainsides [1]. The sedimentation velocity, which is highly influenced by particle size, plays a significant role in the dynamic of turbulent dilute suspension avalanches [1]. Accurate modeling of sedimentation velocity is crucial, as errors in size estimation can lead to 10x greater errors in velocity estimation.

In this report, I analyze a dataset of snow particle sizes obtained from a PhD student at the Laboratory of Cryospheric Sciences at EPFL (Ecole Polytechnique Fédérale de Lausanne). The dataset includes the number of particles in each size category, which is the most important feature for our analysis. I begin by performing exploratory data analysis to gain insights into the dataset and create a preliminary model. I then enhance this model using both local optimization and Bayesian methods. Finally, I assess the model's quality by performing parametric bootstrap analysis.


## 2. Data Characteristics
The outcome of interest is the variable *retained* which is the fraction of particles belonging to each diameter bin. The other available variables are 

+ The startpoint of the bin,
+ The endpoint of the bin,
+ The number of particles detected,

Our study aims to simulate snowflake diameters that accurately reflect the data, in order to investigate the aeolian transport of snow. To this end, we are analyzing the variable 'retained,' which represents the fraction of particles belonging to each diameter bin. 

Figure 1 displays the percentage of particles in each bin, with the distribution plotted according to the bin startpoint. The figure suggests that the data can be described by a mixture of log-normal distributions, with two distinct peaks visible: one at the startpoint of 0.25 and the other at the startpoint of 0.75.
Figure 1 shows the variable of interest ie the percentage of particles in each bin (the distribution is plotted according to the bin startpoint). The plot I obtain allows us to think of a mixture of log-normal distribution with two visible peaks : one at the startpoint 0.25 and one at the startpoint 0.75 approximately. 

```{r, fig.align="center", fig.cap="Figure 1: Percentage of particles in each bin"}
plot(df$startpoint,df$retained...., main ='Percentage of particles in each bin', xlab = 'bin startpoint', ylab ='Frequency(%)', type='l')
```



## 3. Model fitting
After identifying that our data may be distributed as a mixture of two log-normal distributions, our next step is to evaluate the parameters for the density function:

$f(x) = w_1 \frac{1}{x \sigma_1 \sqrt{2\pi}} e^{-\frac{(\ln x - \mu_1)^2}{2\sigma_1^2}} + w_2 \frac{1}{x \sigma_2 \sqrt{2\pi}} e^{-\frac{(\ln x - \mu_2)^2}{2\sigma_2^2}}$

To evaluate the five parameters $w_1$, $\mu_1$, $\sigma_1$, $\mu_2$, and $\sigma_2$, we will use the Expectation-Maximization (EM) algorithm, which is a widely used iterative method for estimating the parameters of statistical models with latent variables.

In the first step of the EM algorithm, we will initialize the parameters and estimate the posterior distribution of the latent variables. In the second step, we will update the parameters using the expected values of the latent variables computed in the first step. We will repeat these two steps until convergence, which typically occurs when the change in the log-likelihood function falls below a certain threshold.

After obtaining an initial estimate of the parameters using the EM algorithm, we will explore other methods to improve the estimation, such as local search and Bayesian approaches.

### EM algorithm 
I implement an EM-algorithm taking as initialisation parameters 

$w_1 = 0.5$, $\mu_1 = 0.25$, $\sigma_1 = 0.75$, $\mu_2 = 0.25$, $\sigma_2 = 0.5$

I define $\epsilon = 0.001$ the threshold such that I stop the algorithm when the difference in the log-likelihoods is under $\epsilon$.

```{r densities, echo=FALSE}
lognormal <- function(x, mu1, sigma1){
  if (x>0){
    return((1/x)*dnorm(log(x), mean=mu1, sd=sigma1))}
  else{
    return(0)
  }
}


bilognorm <- function(x, mu1, mu2, sigma1, sigma2, p) {
    return(p*lognormal(x,mu1,sigma1)+(1-p)*lognormal(x,mu2,sigma2))
}
```

```{r test, echo=FALSE}
test_0 <- function(xn){
  res <-0
  for (i in 1:length(xn)){
    if (xn[i] ==0){
      res <- res + 1
    }
  }
  return(res)}

```

```{r bi log normal EM algo, echo=FALSE}
#Initialisation 
p_init <- 0.5
mu1_init <- 0.25
mu2_init <- 0.75
sigma1_init <- 0.25
sigma2_init <- 0.5
X <- df$retained....

estimate <- function(X, p_init, mu1_init, mu2_init, sigma1_init, sigma2_init){

Y <- numeric(length = length(X))

for (i in 1:length(X)){
  if (X[i]>0){
    Y[i] <- log(X[i])}}

N = length(X)

#criterion : epsilon
epsilon = 0.001
k=0
err = 20000
old_likelihood <- 0 
new_likelihood <- 0 
p_new <- numeric(length=N)

while (err>epsilon){
  
#log likelihood computation
for (i in 1:length(X)){
  old_likelihood <- old_likelihood + bilognorm(X[i], mu1_init, mu2_init, sigma1_init, sigma2_init, p_init)} 
old_likelihood <- log(old_likelihood)

#new parameters
#f_theta <- p_init*dnorm(Y, mean=mu1_init, sd = sigma1_init) + (1-p_init)*dnorm(Y, mean=mu2_init, sd = sigma2_init)

#if (test_0(f_theta) >0 ){
  #gamma <- 0}else {gamma <- p_init*dnorm(Y, mean=mu1_init, sd = sigma1_init)/f_theta}
f_theta <- numeric(length(X))
gamma<- numeric(length(X))
for(i in 1:length(X)){
f_theta[i] <- bilognorm(X[i], mu1_init, mu2_init, sigma1_init, sigma2_init, p_init)
if(f_theta[i]>0){
gamma[i] <- p_init*lognormal(X[i], mu1_init, sigma1_init)/f_theta[i]}else{gamma[i]<-1}}

s <- sum(gamma)
p_new <- s/N
mu_1_new <- sum(Y*gamma)/s
mu_2_new <- sum(Y*(1-gamma))/(N-s)
s1 <- sum(gamma*(Y - mu1_init)**2)
s2 <- sum((1-gamma)*(Y - mu2_init)**2)
sigma1_new <- sqrt(s1/(N-s))
sigma2_new <- sqrt(s2/(N-s))

#new log likelihood computation
for (i in 1:length(X)){
  new_likelihood <- new_likelihood + bilognorm(X[i], mu_1_new, mu_2_new, sigma1_new, sigma2_new, p_new)}
new_likelihood <- log(new_likelihood)

#k represents the number of iterations before convergence
k = k+1

#err is the error 
err = abs(new_likelihood- old_likelihood)

#new parameters
mu1_init <- mu_1_new
mu2_init <- mu_2_new
sigma1_init <- sigma1_new
sigma2_init <- sigma2_new
p_init <- p_new

#I put the likelihood at 0 to be able to compute them again 
old_likelihood <- 0 
new_likelihood <- 0 


}

return(c(mu1_init, mu2_init, sigma1_init, sigma2_init, p_init))}

param <- estimate(X = X, p_init = p_init, mu1_init = mu1_init, mu2_init =mu2_init, sigma1_init = sigma1_init, sigma2_init = sigma2_init )

print(c('The obtained parameters are', param))
```

With the parameters I obtained, I generate Figure2 which shows the histogram of the fitted model and our data repartition.

```{r, fig.align="center", fig.cap="Figure 2: Fitted distribution histogram"}
mu1_init <- param[1]
mu2_init <- param[2]
sigma1_init <- param[3]
sigma2_init <- param[4]
p_init <- param[5]

sample <- numeric(length = 52)
for (i in 1:52){
  sample[i] <- bilognorm(df$retained....[i], mu1_init, mu2_init, sigma1_init, sigma2_init, p_init) }
hist(sample, main ='Histogram of the fitted distribution and data density', xlab='bin startpoint', ylab = 'Number of occurences')
lines(df$startpoint, df$retained....)
#lines(df$retained...., bilognorm(df$retained...., mu1_init, mu2_init, sigma1_init, sigma2_init, p_init))
```

Although the histogram I obtained approximately fits the data, it is not fully satisfying because the first peak at 0 is weaker than in the true distribution, and the peak at 0.6 in our true distribution is not accurately modeled. To improve the fit, we will consider using local optimization techniques.

Local optimization seeks to find the values of the parameters that minimize a given objective function, by searching the parameter space around an initial point.

### Local optimization
First, I want to see how our estimated density behaves when I slightly modify one or several parameters. I define an $\epsilon = 0.1$ that I will add or subtract to my parameters to see to what extent this changes the model. The different setups are the following 

| Setup |  mean 1       |     mean 2  |    standard deviation 1| standard deviation 2     |    probability  |
|---------|--------------|------------|---------------|---------------|----------|
| Setup 1 | $\mu_1$       |     $\mu_2$ |    $\sigma_1$ | $\sigma_2$     |    $p$  |
| Setup 2 | $\mu_1 + \epsilon$ | $\mu_2+\epsilon$ | $\sigma_1+\epsilon$ | $\sigma_2+\epsilon$ | $p+2\epsilon$ |
| Setup 3 | $\mu_1 - \epsilon$ | $\mu_2-\epsilon$ | $\sigma_1-\epsilon$ | $\sigma_2-\epsilon$ | $p+2\epsilon$ |
| Setup 4 | $\mu_1 - \epsilon$ | $\mu_2+\epsilon$ | $\sigma_1+\epsilon$ | $\sigma_2+\epsilon$ | $p+2\epsilon$ |
| Setup 5 | $\mu_1 + \epsilon$ | $\mu_2-\epsilon$ | $\sigma_1$     | $\sigma_2$     | $p+2\epsilon$ |
| Setup 6 | $\mu_1 - \epsilon$ | $\mu_2+\epsilon$ | $\sigma_1$     | $\sigma_2$     |$p+\epsilon$ |

where $\mu_1$, $\sigma_1$, $\mu_2$, $\sigma_2$ and $p$ are the parameters estimated with the EM algorithm. In other words, setup number one replicates uses the parameters from our previous estimation and is used to be compared to the different five other setups. 

The results are given by Figure3. It appears that setup 4 and 2 are improving the model even though the first part of the histogram is still not well modeled. 
In both models, we have chosen a mu2, a sigma1, a sigma2 and p greater than the one obtained with the EM-algorithm. 

```{r, null param, echo=FALSE, include=FALSE}
null_param <- function(param){
  res <-0
  for(i in 1:length(param)){
    if (param[i] <0.1){
      res <- res +1 
    }
  }
  return(res)
}

```


```{r, fig.align="center", fig.cap="Figure 3: Local optimization"}
#values I obtained and that I want to optimize 
mu1 <- mu1_init
mu2 <- mu2_init
sigma1 <- sigma1_init
sigma2 <- sigma2_init
p <- p_init

#I define epsilon from which I will make our values change to see which model is better fitting our data
epsilon = 0.1

par(mfrow = c(2,3))

#graph1
sample <- numeric(length = 52)
for (i in 1:52){
  sample[i] <- bilognorm(df$retained....[i], mu1_init, mu2_init, sigma1_init, sigma2_init, p_init) }
hist(sample, main='Distribution with setup 1')
lines(df$startpoint, df$retained....)

#graph2
sample <- numeric(length = 52)
for (i in 1:52){
  sample[i] <- bilognorm(df$retained....[i], mu1_init+epsilon, mu2_init+epsilon, sigma1_init+epsilon, sigma2_init+epsilon, p_init+2*epsilon) }
hist(sample, main='Distribution with setup 2')
lines(df$startpoint, df$retained....)

#graph3
sample <- numeric(length = 52)
for (i in 1:52){
  sample[i] <- bilognorm(df$retained....[i], mu1_init-epsilon, mu2_init-epsilon, sigma1_init-epsilon, sigma2_init-epsilon, p_init+2*epsilon) }
hist(sample, main='Distribution with setup 3')
lines(df$startpoint, df$retained....)

#graph4
sample <- numeric(length = 52)
for (i in 1:52){
  sample[i] <- bilognorm(df$retained....[i], mu1_init-epsilon, mu2_init+epsilon, sigma1_init+epsilon, sigma2_init+epsilon, p_init+2*epsilon) }
hist(sample, main='Distribution with setup 4')
lines(df$startpoint, df$retained....)

#graph5
sample <- numeric(length = 52)
for (i in 1:52){
  sample[i] <- bilognorm(df$retained....[i], mu1_init+epsilon, mu2_init-epsilon, sigma1_init, sigma2_init, p_init+2*epsilon) }
hist(sample, main='Distribution with setup 5')
lines(df$startpoint, df$retained....)

#graph6
sample <- numeric(length = 52)
for (i in 1:52){
  sample[i] <- bilognorm(df$retained....[i], mu1_init-epsilon, mu2_init+epsilon, sigma1_init, sigma2_init, p_init+epsilon) }
hist(sample, main = 'Distribution with setup 6')
lines(df$startpoint, df$retained....)
```

Now that I have been able to see the behaviour of my distribution when I change the parameters, I do some kernel density estimation. Kernel density estimation is a non-parametric technique used to estimate the probability density function of a random variable based on a sample of data. The idea behind kernel density estimation is to place a "kernel" function at each data point, and then sum the kernel functions to create a smooth estimate of the density function.

Kernel functions are typically symmetric, non-negative functions with zero mean, such as the Gaussian kernel or the Epanechnikov kernel. We then estimate the density function as follows:

$f(x) = \frac{1}{n h} \sum_{i=1}^n K\left(\frac{x - X_i}{h}\right)$

where h is a bandwidth parameter that controls the width of the kernel function and determines the degree of smoothing. A smaller bandwidth results in a more variable estimate, while a larger bandwidth results in a smoother estimate that may oversmooth the data.

Figure 4 represents in red our data distribution and in black the one estimated with the kernel density estimation. I chose the normal kernel with the density $$f(x) = \frac{1}{\sigma \sqrt{2 \pi}} e^{-\frac{(x-\mu)^2}{2\sigma^2}}$$ 

```{r fig.align="center", fig.cap="Figure 4: Kernel density estimation"}
library('KernSmooth')

plot(df$startpoint, df$retained...., type ='l', col = 'red')
pol <- locpoly(df$startpoint, df$retained...., drv = 0, degree =2, kernel = "normal", 
        bandwidth = 0.25 )
lines(pol)

```

After re-evaluating the parameters, we refit the model to see if there were any improvements. Figure 5 shows the results, which indicate a slight improvement in the histogram thanks to the use of kernel density estimation. Specifically, we see that the peak at 0.6 is better estimated, suggesting that our modeling approach is capturing more of the underlying distribution. However, we note that the peak at 0 is still over-estimated, which indicates that further optimization may be necessary to obtain an accurate model.

```{r optimization 3, echo=FALSE}
#p_init <- 0.5
#mu1_init <- 0.25
#mu2_init <- 0.75
#sigma1_init <- 0.25
#sigma2_init <- 0.5
new_X <- unname(unlist(pol[2]))

estimate_2 <- function(new_X, p_init, mu1_init, mu2_init, sigma1_init, sigma2_init ){
Y <- numeric(length = length(new_X))

for (i in 1:length(new_X)){
  if (new_X[i]>0){
    Y[i] <- log(new_X[i])}}

N = length(new_X)

#criterion : epsilon 
epsilon = 0.001
k=0
err = 20000
old_likelihood <- 0 
new_likelihood <- 0 
p_new <- numeric(length=N)
param <- c(1,1,1,1,1)

while(null_param(param)==0){
if(err>epsilon){
  #log likelihood computation
for (i in 1:length(new_X)){
  if(new_X[i]<0){
    new_X[i] = 0}
  a <- bilognorm(new_X[i], mu1_init, mu2_init, sigma1_init, sigma2_init, p_init)
  if (is.na(a)==FALSE){
    old_likelihood <- old_likelihood + a}}
old_likelihood <- log(old_likelihood)

#new parameters
#f_theta <- p_init*dnorm(Y, mean=mu1_init, sd = sigma1_init) + (1-p_init)*dnorm(Y, mean=mu2_init, sd = sigma2_init)

#if (test_0(f_theta) >0 ){
  #gamma <- 0}else {gamma <- p_init*dnorm(Y, mean=mu1_init, sd = sigma1_init)/f_theta}
f_theta <- numeric(length(new_X))
gamma<- numeric(length(new_X))
for(i in 1:length(new_X)){
  f_theta[i] <- bilognorm(new_X[i], mu1_init, mu2_init, sigma1_init, sigma2_init, p_init)
 if(f_theta[i]>0){
gamma[i] <- p_init*lognormal(new_X[i], mu1_init, sigma1_init)/f_theta[i]}else{gamma[i] =  1}}

s <- sum(gamma)
p_new <- s/N
mu_1_new <- sum(Y*gamma)/s
mu_2_new <- sum(Y*(1-gamma))/(N-s)
s1 <- sum(gamma*(Y - mu1_init)**2)
s2 <- sum((1-gamma)*(Y - mu2_init)**2)
sigma1_new <- sqrt(s1/(N-s))
sigma2_new <- sqrt(s2/(N-s))

#new log likelihood computation
for (i in 1:length(X)){
  if(new_X[i]<0){
    new_X[i] = abs(new_X[i])}
  a <- bilognorm(new_X[i], mu1_init, mu2_init, sigma1_init, sigma2_init, p_init)
  if (is.na(a)==FALSE){
    new_likelihood <- new_likelihood + a}}
new_likelihood <- log(new_likelihood)

#k represents the number of iterations before convergence
k = k+1

#err is the error 
err = abs(new_likelihood - old_likelihood)

#new parameters
mu1_init <- mu_1_new
mu2_init <- mu_2_new
sigma1_init <- sigma1_new
sigma2_init <- sigma2_new
p_init <- p_new

param <- c(mu1_init,mu2_init,sigma1_init,sigma2_init,p_init)
#I put the likelihood at 0 to be able to compute them again 
old_likelihood <- 0 
new_likelihood <- 0 

}}
  return(param)
}

new_param <- estimate_2(new_X, p_init, mu1_init, mu2_init, sigma1_init, sigma2_init)

print(c('The obtained values are', new_param))
```


```{r, fig.align="center", fig.cap="Figure 5: Kernel density estimation results"}
par(mfrow=c(1,2))
sample <- numeric(length = 52)
for (i in 1:52){
  sample[i] <- bilognorm(df$retained....[i], new_param[2], new_param[3], new_param[4], new_param[5], new_param[1]) }
hist(sample, main='Fitted model with KDE')
lines(df$startpoint, df$retained....)

sample <- numeric(length = 52)
for (i in 1:52){
  sample[i] <- bilognorm(df$retained....[i], mu1, mu2, sigma1, sigma2, p) }
hist(sample, main='Fitted model with EM algorithm')
lines(df$startpoint, df$retained....)
```


### Bayesian approach
In this section, I use Markov chain Monte Carlo (MCMC) to further refine the parameters obtained with local search. MCMC is a powerful technique for estimating the parameters of complex models. The idea behind MCMC is to construct a Markov chain whose stationary distribution is the distribution of interest. This is achieved by iteratively proposing new parameter values and accepting or rejecting them based on the likelihood of the data given the proposed values. The result is a sequence of parameter values that, after a certain number of iterations, approximate the posterior distribution of the parameters.

To implement MCMC, I use a proposal distribution that generates new parameter values from the current value. In my case, I use a Gaussian proposal distribution with a standard deviation of 0.01 and an initial mean given by the parameters obtained in the previous section. The advantage of this proposal is that it is symmetric, which simplifies the acceptance/rejection step of the MCMC algorithm. 

```{r, MCMC, echo=FALSE}
likelihood_mixed <- function(xn, mu1, mu2, sigma1, sigma2, p){
  old_likelihood <- 0
  for (i in 1:length(xn)){
    old_likelihood<- old_likelihood + bilognorm(xn[i], mu1, mu2, sigma1, sigma2, p)
  return(old_likelihood)}}


n_iter_max <- 11000
burn_in <- 1000 
thin <- 2

prob_stock <- rep(0, n_iter_max)

#initialize the parameters 
mu1 <- new_param[2]
mu2 <- new_param[3]
sigma1 <- new_param[4]
sigma2 <- new_param[5]
p <- new_param[1]
params <- c(mu1, mu2, sigma1, sigma2, p)

chain <- matrix(0, nrow = n_iter_max, ncol = length(params))
chain[1,] <- params

prop_sd <- c(0.01, 0.01, 0.01, 0.01, 0.01)

for (i in 2:n_iter_max) {
  # Generate proposal
  prop <- rnorm(length(params), mean = chain[i-1,], sd = prop_sd)
  prop <- abs(prop)
  
  # Compute acceptance probability
  #p_prop <- sum(dnorm(X, mean = c(prop[1], prop[2], prop[3], prop[4], prop[5]), sd=prop_sd))
  p_prop = 1
  p_curr = 1 # I have a gaussian proposal that is symmetric so the ratio of p_prop/p_curr is 1 (I set both at 1 to have the ratio equal to 1) 
  #p_curr <- sum(dnorm(X, mean = chain[i-1,], sd = prop_sd))
  accept_prob <- likelihood_mixed(X, prop[1], prop[2], prop[3], prop[4], prop[5])*p_prop/(likelihood_mixed(X, chain[i-1,][1], chain[i-1,][2], chain[i-1,][3], chain[i-1,][4], chain[i-1,][5])*p_curr)
  if(accept_prob>=1){
    accept_prob <- 1
  }
  prob_stock[i] <- accept_prob
  
  # Decide whether to accept or reject proposal
  if (runif(1) < accept_prob) {
    chain[i,] <- prop
  } else {
    chain[i,] <- chain[i-1,]
  }
}

posterior <- chain[-(1:burn_in),]
posterior <- posterior[seq(1, nrow(posterior), thin),]

```


Figure 6 allows us to see the new fitted distribution which doesn't appear better than the previous one. I discuss those results in the following section.

```{r, fig.align="center", fig.cap="Figure 6: MCMC results"}
sample <- numeric(length = 52)
for (i in 1:52){
  sample[i] <- bilognorm(df$retained....[i], prop[1], prop[2], prop[3], prop[4], prop[5]) }
hist(sample, main='Fitted model with a MCMC method', xlab = 'sample values', ylab ='number of occurences')
lines(df$startpoint, df$retained....)
``` 


## 4. Results analysis
### Convergence of the MCMC 

In Figure 7, the first five plots show the values of the parameters after removing the burn-in period (the first 1000 values in this case). If the MCMC algorithm is working well, we would expect to see convergence of the parameter values over time, indicating that the chain has explored the parameter space and settled into a stable distribution. However, for all of the parameters, we do not see clear convergence over the course of the chain.

The last plot in Figure 7 shows the acceptance probability of the proposals, with the mean value shown in red. Ideally, the acceptance probability should be in the range of 10% to 50%, indicating that the proposals are exploring the space effectively. However, in our case, the acceptance probability is quite high, indicating that the chain is not exploring the space efficiently.

These issues with the MCMC procedure likely explain why our parameter estimates do not appear to have improved in Figure 6. We need to investigate further to determine the cause of these problems and identify strategies for improving the performance of the MCMC algorithm.


```{r fig.align="center", fig.cap="Figure 7: MCMC convergence"}
par(mfrow = c(2,3))
plot(posterior[,1], type="l", main ='Values of mu1', xlab = 'iteration', ylab ='mu1')
plot(posterior[,2], type="l", main ='Values of mu2', xlab = 'iteration', ylab ='mu2')
plot(posterior[,3], type="l", main ='Values of sigma1', xlab = 'iteration', ylab ='sigma1')
plot(posterior[,4], type="l", main ='Values of sigma2', xlab = 'iteration', ylab ='sigma2')
plot(posterior[,5], type="l", main ='Values of p', xlab = 'iteration', ylab ='p')
plot(prob_stock, type="l", main ='Acceptance probability', xlab = 'iteration', ylab ='Acceptance probability')
abline(h = mean(prob_stock), col='red')
```


### Parametric bootstrap
In this section, a parametric bootstrap algorithm [2] is implemented to assess the goodness of fit of the fitted distribution. The purpose of the algorithm is to sample from the fitted distribution using the estimated parameters, and then calculate the Kolmogorov-Smirnov statistic, denoted by $T$. To do so, I sample from the fitted distribution with our estimated parameters. I use the Kolmogorov-Smirnov statistic $T = \sup_x \Big| \widehat{F}_N(x) - F_\widehat{\lambda}(x) \Big|$, where I suppose that under $H_0$ our estimated parameters $\widehat{\lambda}$ are consistent to the real parameters ${\lambda}_0$.

The null hypothesis we want to test is thus 

$H_0:F \in \mathcal{F} = \{ F_\lambda \mid \lambda \in \Lambda \}$ against $H_1: F \notin \mathcal{F}$ if $\mathcal{F}=\{F_0 \}$

To execute the parametric bootstrap algorithm, the procedure outlined in [2] is followed. Resamples are generated, and the parameters are estimated on these new resamples. The empirical distribution function is then calculated, allowing for the calculation of the Kolmogorov-Smirnov statistic. The p-value of the test can then be computed using the T statistic.


```{r parametric bootstrap 2, echo=FALSE}
T_stat <- function(xn, mu1, mu2, sigma1, sigma2,p){
  F_N <- ecdf(xn) #the ecdf of our sample
  n = length(xn) #size of the sample
  evaluate_diff <- runif(n, min(xn), max(xn)) 
  sample <- rep(0,n)
  for (i in 1:n){
    sample[i] <- bilognorm(evaluate_diff[i], mu1, mu2, sigma1, sigma2, p)
  }
  F_star <- ecdf(sample)
  #difabs <- abs(F_N(grid) - plnormMix(grid, meanlog1 = abs(mu1), sdlog1 = abs(sigma1), meanlog2 = abs(mu2), sdlog2 = abs(sigma2), p.mix = p) )
  difabs <- abs(F_N(evaluate_diff) - F_star(evaluate_diff) )
  return(max(difabs))
}


#we use the parameters we estimated before
mu1_boot <- new_param[2]
mu2_boot <- new_param[3]
sigma1_boot <- new_param[4]
sigma2_boot <- new_param[5]
p_boot <- new_param[1]
T_0 <- T_stat(X, mu1_boot, mu2_boot, sigma1_boot, sigma2_boot, p_boot) #ok 

Bootstrap <- function(xn, T_0, mu1, mu2, sigma1, sigma2, p, B){
  n = length(xn)
  s <- 0
  
  #Bootstrap
  T_boot <- rep(NA, B)
  x_resamples <- matrix(NA, nrow = B, ncol = n) #to put our resamples
  Y <- matrix(NA, nrow = B, ncol = n) 
  for (i in 1:B) {
    x_resamples[i,] <- sample(xn, replace = TRUE)
  }
  for(i in 1:B){
    #Y <- numeric(length = length(x_resamples[i,]))
    for (k in 1:length(x_resamples[i,])){
    if (x_resamples[i,][k]>0){
      Y[i] <- log(x_resamples[i,][k])}}

    param = estimate(x_resamples[i,], p, mu1, mu2, sigma1, sigma2)
    mu1_estim <- param[1]
    mu2_estim <- param[2]
    sigma1_estim <- param[3]
    sigma2_estim <- param[4]
    p_estim <- param[5]
    T_boot[i] <- T_stat(x_resamples[i,], mu1_estim, mu2_estim, sigma1_estim, sigma2_estim, p_estim) 
  }
  for (i in 1:length(T_boot)){
    if (T_boot[i]>T_0){
      s <- s+1
    }
  }
  return((1/(B+1))*(1 + s))
}

Bootstrap(X, T_0, mu1, mu2, sigma1, sigma2, p, 100)
#b <- numeric(5)
#for(i in 1:5){
  #b[i] <- Bootstrap(X, T_0, mu1, mu2, sigma1, sigma2, p, 50)}
#boxplot(b, main ='P-value of the parametric bootstrap boxplot')
```

The obtained p-value is of 0.56 which is big enough to consider that we don't reject the null hypothesis. In other words, we conclude that the hypothesis that our data comes from a mixture of two lognormal distributions is not rejected.

## 6. Discussion and conclusion
Throughout this project, I have focused on the mixture of two lognormal distributions as it fits the data well, and our parametric bootstrap algorithm did not reject it. Estimating the parameters of this distribution is challenging, as there is no closed-form formula for the maximum likelihood estimate. I have used a combination of the EM algorithm and local optimization to estimate the parameters, which resulted in a slightly improved fit of the model. The initial parameters for the EM algorithm were chosen based on descriptive statistics and provided a satisfactory starting point for the optimization.

Although the MCMC procedure was expected to further refine the parameter estimates, it failed to converge. The acceptance probability was high, indicating that the parameters space was not adequately explored. To improve the algorithm, I could try using a proposal distribution other than the Gaussian or adjust the standard deviation of the parameters to explore more of the space. Overall, this project demonstrates the importance of using multiple methods to estimate parameters and assess the goodness of fit of a distribution.

## 7. References
[1] https://hal.inria.fr/inria-00405714/document \\

[2] https://github.com/TMasak/StatComp/blob/master/Notes/10_Bootstrap.Rmd, section 'Parametric bootstrap and goodness of fit' \\

[3]https://github.com/TMasak/StatComp/blob/master/Slides/13_BayesComp.pdf 
