---
title: "Report"
author: "Sciper 359523"
date: "2023-02-21"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message=FALSE, warning=FALSE)
```

```{r working directory and data, echo=FALSE, include=FALSE}
lapply(c("dplyr","chron","ggplot2","tidyr","questionr","survival","forcats","tidyselect",
         "data.table","table1","lubridate", "ggpubr","viridis","finalfit","survminer",
         "ggpubr", "ggthemes", "gridExtra", "rstatix","stringr",
         "wesanderson","kableExtra", "naniar","boot","scales","ggsci", "stringr",
         "Hmisc","DescTools","swimplot", 'stats', 'EnvStats'), 
       library, character.only=TRUE)

Sys.setlocale("LC_TIME", "English")
df <- read.csv("1_snow_particles.csv")
```


## 1. Introduction
trouver une intro

In this report, we work with a data set TO DO 


## 2. Data Characteristics
The outcome of interest is the variable *retained* which is the fraction of particles belonging to each diameter bin. The other variables we have are 

+ The startpoint of the bin,
+ The endpoint of the bin,
+ The number of particles detected,

Our goal is to simulate snow-flake diameters that are compatible with the data in order to study aeolian transport of snow. 

Figure 1 shows the variable of interest ie the percentage of particles in each bin (the distribution is plotted according to the bin startpoint). The plot we obtain allows us to think of a mixture of log-normal distribution with two visible peaks : one at the bin 11 and one at the bin 30. 

```{r, fig.align="center", fig.cap="Figure 1: Percentage of particles in each bin"}
plot(df$startpoint,df$retained..../100, main ='Percentage of particles in each bin', xlab = 'bin startpoint', ylab ='Frequency(%)', type='l')
```



## 3. Model fitting
Now that we identified graphically that our data is possibly distributed as a mixture of two log normal distributions, we want to evaluate the parameters. The density is 
$f(x) = w_1 \frac{1}{x \sigma_1 \sqrt{2\pi}} e^{-\frac{(\ln x - \mu_1)^2}{2\sigma_1^2}} + w_2 \frac{1}{x \sigma_2 \sqrt{2\pi}} e^{-\frac{(\ln x - \mu_2)^2}{2\sigma_2^2}}$

which means that we have the following five parameters to evaluate 

$w_1$,$mu_1$,$sigma_1$,$mu_2$,$sigma_2$,

We use an EM-algorithm to do a first evaluation of those parameters, and then we will try to improve our estimation with local search and with a bayesian approach.

### EM algorithm 
We implement an EM-algorithm taking as initialisation parameters 

$w_1 = 0.5$, $mu_1 = 0.25$, $sigma_1 = 0.75$, $mu_2 = 0.25$, $sigma_2 = 0.5$

We define $epsilon = 0.001$ the threshold such that we stop the algorithm when the difference in the log-likelihoods is under $epsilon$.

```{r densities, echo=FALSE}
lognormal <- function(x, mu1, sigma1){
  if (x!=0){
    return((1/x)*dnorm(log(x), mean=mu1, sd=sigma1))}
  else{
    return(0)
  }
}


bilognorm <- function(x, mu1, mu2, sigma1, sigma2, p) {
    return(p*lognormal(x,mu1,sigma1)+(1-p)*lognormal(x,mu2,sigma2))
}
```

```{r bi log normal EM algo, echo=FALSE}
#Initialisation 
p_init <- 0.5
mu1_init <- 0.25
mu2_init <- 0.75
sigma1_init <- 0.25
sigma2_init <- 0.5
X <- df$retained....

estimate <- function(X, p_init, mu1_init, mu2_init, sigma1_init, sigma2_init){

Y <- numeric(length = length(X))

for (i in 1:length(X)){
  if (X[i]!=0){
    Y[i] <- log(X[i])}}

N = length(X)

#criterion : epsilon 
eps = 0.001
k=0
err = 20000
old_likelihood <- 0 
new_likelihood <- 0 
p_new <- numeric(length=N)

while (err>eps){
  
#log likelihood computation
for (i in 1:length(X)){
  old_likelihood <- old_likelihood + bilognorm(X[i], mu1_init, mu2_init, sigma1_init, sigma2_init, p_init)} 
old_likelihood <- log(old_likelihood)

#new parameters
f_theta <- p_init*dnorm(Y, mean=mu1_init, sd = sigma1_init) + (1-p_init)*dnorm(Y, mean=mu2_init, sd = sigma2_init)

gamma <- p_init*dnorm(Y, mean=mu1_init, sd = sigma1_init)/f_theta

s <- sum(gamma)
p_new <- s/N
mu_1_new <- sum(Y*gamma)/s
mu_2_new <- sum(Y*(1-gamma))/(N-s)
s1 <- sum(gamma*(Y - mu1_init)**2)
s2 <- sum((1-gamma)*(Y - mu2_init)**2)
sigma1_new <- sqrt(s1/(N-s))
sigma2_new <- sqrt(s2/(N-s))

#new log likelihood computation
for (i in 1:length(X)){
  new_likelihood <- new_likelihood + bilognorm(X[i], mu_1_new, mu_2_new, sigma1_new, sigma2_new, p_new)}
new_likelihood <- log(new_likelihood)

#k represents the number of iterations before convergence
k = k+1

#err is the error 
err = abs(new_likelihood- old_likelihood)

#new parameters
mu1_init <- mu_1_new
mu2_init <- mu_2_new
sigma1_init <- sigma1_new
sigma2_init <- sigma2_new
p_init <- p_new

#we put the likelihood at 0 to be able to compute them again 
old_likelihood <- 0 
new_likelihood <- 0 

}

return(c(mu1_init, mu2_init, sigma1_init, sigma2_init, p_init))}

param <- estimate(X = X, p_init = p_init, mu1_init = mu1_init, mu2_init =mu2_init, sigma1_init = sigma1_init, sigma2_init = sigma2_init )

print(c('The obtained parameters are', param))
```

With the parameters we obtained, we generate Figure2. With the parameters we estimated, the figure shows the histogram we obtain and the data repartition above to compare. 

```{r, fig.align="center", fig.cap="Figure 2: Fitted distribution histogram"}
mu1_init <- param[1]
mu2_init <- param[2]
sigma1_init <- param[3]
sigma2_init <- param[4]
p_init <- param[5]

sample <- numeric(length = 52)
for (i in 1:52){
  sample[i] <- bilognorm(df$retained....[i], mu1_init, mu2_init, sigma1_init, sigma2_init, p_init) }
hist(sample, main ='Histogram of the fitted distribution and data density', xlab='bin startpoint', ylab = 'Number of occurences')
lines(df$startpoint, df$retained....)
#lines(df$retained...., bilognorm(df$retained...., mu1_init, mu2_init, sigma1_init, sigma2_init, p_init))
```

The obtained histogram is not fully satisfactory because the peak at 0.6 in the data density is not present in the histogram. 

We decide to do some local optimization in order to improve this fit. 

### Local optimization
The considered setup here is that I defined an $epsilon = 0.1$ that I will add or subtract to my parameters to see to what extent this improves the model. The different setups are the following 

| Setup |  mean 1       |     mean 2  |    standard deviation 1| standard deviation 2     |    probability  |
|---------|--------------|------------|---------------|---------------|----------|
| Setup 1 | $mu_1$       |     $mu_2$  |    $sigma_1$ | $sigma_2$     |    $p$  |
| Setup 2 | $mu_1 + eps$ | $mu_2+eps$ | $sigma_1+eps$ | $sigma_2+eps$ | $p+2eps$ |
| Setup 3 | $mu_1 - eps$ | $mu_2-eps$ | $sigma_1-eps$ | $sigma_2-eps$ | $p+2eps$ |
| Setup 4 | $mu_1 - eps$ | $mu_2+eps$ | $sigma_1+eps$ | $sigma_2+eps$ | $p+2eps$ |
| Setup 5 | $mu_1 + eps$ | $mu_2-eps$ | $sigma_1$     | $sigma_2$     | $p+2eps$ |
| Setup 6 | $mu_1 - eps$ | $mu_2+eps$ | $sigma_1$     | $sigma_2$     |$p+eps$ |

where $mu_1$, $sigma_1$, $mu_2$, $sigma_2$ and $p$ are the parameters estimated with the EM algorithm. In other words, setup number one replicates uses the parameters from our previous estimation and is used to be compared to the different five other setups. 

The results are given by Figure3. It appears that setup 4 and 2 are improving the model even though the first part of the histogram is way larger than it should be still.

```{r, fig.align="center", fig.cap="Figure 3: Local optimization"}
#values we obtained and that we want to optimize 
mu1 <- mu1_init
mu2 <- mu2_init
sigma1 <- sigma1_init
sigma2 <- sigma2_init
p <- p_init

#we define eps from which we will make our values change to see which model is better fitting our data
eps = 0.1

par(mfrow = c(2,3))

#graph1
sample <- numeric(length = 52)
for (i in 1:52){
  sample[i] <- bilognorm(df$retained....[i], mu1_init, mu2_init, sigma1_init, sigma2_init, p_init) }
hist(sample, main='Distribution with setup 1')
lines(df$startpoint, df$retained....)

#graph2
sample <- numeric(length = 52)
for (i in 1:52){
  sample[i] <- bilognorm(df$retained....[i], mu1_init+eps, mu2_init+eps, sigma1_init+eps, sigma2_init+eps, p_init+2*eps) }
hist(sample, main='Distribution with setup 2')
lines(df$startpoint, df$retained....)

#graph3
sample <- numeric(length = 52)
for (i in 1:52){
  sample[i] <- bilognorm(df$retained....[i], mu1_init-eps, mu2_init-eps, sigma1_init-eps, sigma2_init-eps, p_init+2*eps) }
hist(sample, main='Distribution with setup 3')
lines(df$startpoint, df$retained....)

#graph4
sample <- numeric(length = 52)
for (i in 1:52){
  sample[i] <- bilognorm(df$retained....[i], mu1_init-eps, mu2_init+eps, sigma1_init+eps, sigma2_init+eps, p_init+2*eps) }
hist(sample, main='Distribution with setup 4')
lines(df$startpoint, df$retained....)

#graph5
sample <- numeric(length = 52)
for (i in 1:52){
  sample[i] <- bilognorm(df$retained....[i], mu1_init+eps, mu2_init-eps, sigma1_init, sigma2_init, p_init+2*eps) }
hist(sample, main='Distribution with setup 5')
lines(df$startpoint, df$retained....)

#graph6
sample <- numeric(length = 52)
for (i in 1:52){
  sample[i] <- bilognorm(df$retained....[i], mu1_init-eps, mu2_init+eps, sigma1_init, sigma2_init, p_init+eps) }
hist(sample, main = 'Distribution with setup 6')
lines(df$startpoint, df$retained....)
```


### Bayesian approach
We use in this section MCMC (Markov chain Monte Carlo) to approximate better the parameters obtained with the local search. The proposal we use is the Gaussian distribution with standard deviation 0.01 and initial mean given by the parameters obtained in the previous section. 

```{r MCMC, echo=FALSE}
likelihood_mixed <- function(xn, mu1, mu2, sigma1, sigma2, p){
  old_likelihood <- 0
  for (i in 1:length(xn)){
    old_likelihood<- old_likelihood + bilognorm(xn[i], mu1, mu2, sigma1, sigma2, p)
  return(old_likelihood)}}


n_iter_max <- 11000
burn_in <- 1000 
thin <- 2

#initialize the parameters 
mu1 <- mu1_init-eps
mu2 <- mu2_init+eps
sigma1 <- sigma1_init+eps
sigma2 <- sigma2_init+eps
p <- p_init+2*eps
params <- c(mu1, mu2, sigma1, sigma2, p)

chain <- matrix(0, nrow = n_iter_max, ncol = length(params))
chain[1,] <- params

prop_sd <- c(0.01, 0.01, 0.01, 0.01, 0.01)

for (i in 2:n_iter_max) {
  # Generate proposal
  prop <- rnorm(length(params), mean = chain[i-1,], sd = prop_sd)
  prop <- abs(prop)
  
  # Compute acceptance probability
  #p_prop <- sum(dnorm(X, mean = c(prop[1], prop[2], prop[3], prop[4], prop[5]), sd=prop_sd))
  p_prop = 1
  p_curr = 1 # we have a gaussian proposal that is symmetric so the ratio of p_prop/p_curr is 1 (I set both at 1 to have the ratio equal to 1) 
  #p_curr <- sum(dnorm(X, mean = chain[i-1,], sd = prop_sd))
  accept_prob <- likelihood_mixed(X, prop[1], prop[2], prop[3], prop[4], prop[5])*p_prop/(likelihood_mixed(X, chain[i-1,][1], chain[i-1,][2], chain[i-1,][3], chain[i-1,][4], chain[i-1,][5])*p_curr)
  
  # Decide whether to accept or reject proposal
  if (runif(1) < accept_prob) {
    chain[i,] <- prop
  } else {
    chain[i,] <- chain[i-1,]
  }
}

posterior <- chain[-(1:burn_in),]
posterior <- posterior[seq(1, nrow(posterior), thin),]

```


Figure 4 allows us to see the new fitted distribution which doesn't appear better than the previous one. 

```{r, fig.align="center", fig.cap="Figure 4: MCMC results"}
sample <- numeric(length = 52)
for (i in 1:52){
  sample[i] <- bilognorm(df$retained....[i], prop[1], prop[2], prop[3], prop[4], prop[5]) }
hist(sample, main='', xlab = 'sample values', ylab ='number of occurences')
lines(df$startpoint, df$retained....)
``` 


## 4. Results analysis
We study in this section our results. TO DO 
### Convergence of the MCMC 
```{r convergence, echo=FALSE}
par(mfrow = c(2,3))
for (i in 1:5) {
  plot(posterior[,i], type="l", main ='', xlab = 'iteration', ylab ='')
}
```


### Parametric bootstrap
We implement in this section a parametric bootstrap algorithm [] to assess the goodness of fit of our distribution. To do so, we sample from the fitted distribution with our estimated parameters. We use the Kolmogorov-Smirnov statistic $T = \sup_x \Big| \widehat{F}_N(x) - F_\widehat{\lambda}(x) \Big|$, where we suppose that under $H_0$ our estimated parameters $\widehat{\lambda}$ are consistent to the real parameters ${\lambda}_0$.

We then generate resamples and estimate the parameters on those new resamples. We are then able to compute the empirical distribution function and therefore compute the T statistic. We can then compute the p-value of the test.







### Interpretation


## 6. Discussion





## 7. References
[] https://github.com/TMasak/StatComp/blob/master/Notes/10_Bootstrap.Rmd, section 'Parametric bootstrap and goodness of fit' 
