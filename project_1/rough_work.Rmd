---
title: "candice applied stat"
output: html_document
date: "2023-02-20"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r working directory and data, echo=FALSE, include=FALSE}
lapply(c("dplyr","chron","ggplot2","tidyr","questionr","survival","forcats","tidyselect",
         "data.table","table1","lubridate", "ggpubr","viridis","finalfit","survminer",
         "ggpubr", "ggthemes", "gridExtra", "rstatix","stringr",
         "wesanderson","kableExtra", "naniar","boot","scales","ggsci", "stringr",
         "Hmisc","DescTools","swimplot", 'stats', 'EnvStats'), 
       library, character.only=TRUE)

Sys.setlocale("LC_TIME", "English")
df <- read.csv("1_snow_particles.csv")
```

# Exploratory data analysis
I start by doing a simple exploratory data analysis by plotting the points. 
```{r exploratory, echo=FALSE}
plot(df$X,df$retained..../100)
df$binsize <- df$endpoint - df$startpoint

plot(log(df$retained..../100))

library('EnvStats')
hist(rlnormMix(52, meanlog1 = log(10), sdlog1 = 0.5, meanlog2 = log(35), sdlog2 = 1, p.mix = 0.2), freq = F )
lines(df$X,df$retained..../100)

```

```{r densities, echo=FALSE}
lognormal <- function(x, mu1, sigma1){
  if (x>0){
    return((1/x)*dnorm(log(x), mean=mu1, sd=sigma1))}
  else{
    return(0)
  }
}


bilognorm <- function(x, mu1, mu2, sigma1, sigma2, p) {
    return(p*lognormal(x,mu1,sigma1)+(1-p)*lognormal(x,mu2,sigma2))
}
```

# Fit a mixture of log normal distributions
## EM algorithm

```{r test_0, echo=FALSE}
test_0 <- function(xn){
  res <-0
  for (i in 1:length(xn)){
    if (xn[i] ==0){
      res <- res + 1
    }
  }
  return(res)}

```

```{r bi log normal EM algo, echo=FALSE}
#Initialisation 
p_init <- 0.5
mu1_init <- 0.25
mu2_init <- 0.75
sigma1_init <- 0.25
sigma2_init <- 0.5
X <- df$retained....

estimate <- function(X, p_init, mu1_init, mu2_init, sigma1_init, sigma2_init){


Y <- numeric(length = length(X))


for (i in 1:length(X)){
  if (X[i]!=0){
    Y[i] <- log(X[i])}}

N = length(X)

#criterion : epsilon 
eps = 0.001
k=0
err = 20000
old_likelihood <- 0 
new_likelihood <- 0 
p_new <- numeric(length=N)

while (err>eps){
  
#log likelihood computation
for (i in 1:length(X)){
  old_likelihood <- old_likelihood + bilognorm(X[i], mu1_init, mu2_init, sigma1_init, sigma2_init, p_init)} 
old_likelihood <- log(old_likelihood)

#new parameters
f_theta <- p_init*dnorm(Y, mean=mu1_init, sd = sigma1_init) + (1-p_init)*dnorm(Y, mean=mu2_init, sd = sigma2_init)

if (test_0(f_theta) >0 ){
   gamma <- rep(0, n)
}
else {gamma <- p_init*dnorm(Y, mean=mu1_init, sd = sigma1_init)/f_theta}

s <- sum(gamma)
p_new <- s/N
mu_1_new <- sum(Y*gamma)/s
mu_2_new <- sum(Y*(1-gamma))/(N-s)
s1 <- sum(gamma*(Y - mu1_init)**2)
s2 <- sum((1-gamma)*(Y - mu2_init)**2)
sigma1_new <- sqrt(s1/(N-s))
sigma2_new <- sqrt(s2/(N-s))

#new log likelihood computation
for (i in 1:length(X)){
  new_likelihood <- new_likelihood + bilognorm(X[i], mu_1_new, mu_2_new, sigma1_new, sigma2_new, p_new)}
new_likelihood <- log(new_likelihood)

#k represents the number of iterations before convergence
k = k+1

#err is the error 
err = abs(new_likelihood- old_likelihood)

#new parameters
mu1_init <- mu_1_new
mu2_init <- mu_2_new
sigma1_init <- sigma1_new
sigma2_init <- sigma2_new
p_init <- p_new

#we put the likelihood at 0 to be able to compute them again 
old_likelihood <- 0 
new_likelihood <- 0 


}

return(c(mu1_init, mu2_init, sigma1_init, sigma2_init, p_init))}

param <- estimate(X = X, p_init = p_init, mu1_init = mu1_init, mu2_init =mu2_init, sigma1_init = sigma1_init, sigma2_init = sigma2_init )

print(c('The obtained parameters are', param))
```

I plot below my data points and the the density I obtain with the estimated parameters.

```{r bi log normal fit, echo=FALSE}
mu1_init <- param[1]
mu2_init <- param[2]
sigma1_init <- param[3]
sigma2_init <- param[4]
p_init <- param[5]

sample <- numeric(length = 52)
for (i in 1:52){
  sample[i] <- bilognorm(df$retained....[i], mu1_init, mu2_init, sigma1_init, sigma2_init, p_init) }
hist(sample)
lines(df$startpoint, df$retained....)
#lines(df$retained...., bilognorm(df$retained...., mu1_init, mu2_init, sigma1_init, sigma2_init, p_init))
```

## Optimization
We want in this section to optimize our parameters estimation. 
```{r optimization, echo=FALSE}
#values we obtained and that we want to optimize 
mu1 <- mu1_init
mu2 <- mu2_init
sigma1 <- sigma1_init
sigma2 <- sigma2_init
p <- p_init

#we define eps from which we will make our values change to see which model is better fitting our data
eps = 0.1

par(mfrow = c(3,2))

#graph1
sample <- numeric(length = 52)
for (i in 1:52){
  sample[i] <- bilognorm(df$retained....[i], mu1_init, mu2_init, sigma1_init, sigma2_init, p_init) }
hist(sample, main='graph1')
lines(df$startpoint, df$retained....)

#graph2
sample <- numeric(length = 52)
for (i in 1:52){
  sample[i] <- bilognorm(df$retained....[i], mu1_init+eps, mu2_init+eps, sigma1_init+eps, sigma2_init+eps, p_init+2*eps) }
hist(sample, main='graph2')
lines(df$startpoint, df$retained....)

#graph3
sample <- numeric(length = 52)
for (i in 1:52){
  sample[i] <- bilognorm(df$retained....[i], mu1_init-eps, mu2_init-eps, sigma1_init-eps, sigma2_init-eps, p_init+2*eps) }
hist(sample, main='graph3')
lines(df$startpoint, df$retained....)

#graph4
sample <- numeric(length = 52)
for (i in 1:52){
  sample[i] <- bilognorm(df$retained....[i], mu1_init-eps, mu2_init+eps, sigma1_init+eps, sigma2_init+eps, p_init+2*eps) }
hist(sample, main='graph4')
lines(df$startpoint, df$retained....)

#graph5
sample <- numeric(length = 52)
for (i in 1:52){
  sample[i] <- bilognorm(df$retained....[i], mu1_init-eps, mu2_init+2*eps, sigma1_init, sigma2_init, p_init+2*eps) }
hist(sample, main='graph5')
lines(df$startpoint, df$retained....)

#graph6
sample <- numeric(length = 52)
for (i in 1:52){
  sample[i] <- bilognorm(df$retained....[i], mu1_init-eps, mu2_init+2*eps, sigma1_init + eps, sigma2_init, p_init+2*eps) }
hist(sample, main = 'graph6')
lines(df$startpoint, df$retained....)
```
```{r optimization 2, echo=FALSE}
library('KernSmooth')

plot(df$startpoint, df$retained...., type ='l', col = 'red')
pol <- locpoly(df$startpoint, df$retained...., drv = 0, degree =2, kernel = "normal", 
        bandwidth = 0.25 )
lines(pol)

```

If we try to estimate only a 'simple' lognormal.
```{r test optim, echo=FALSE}
MLE_lognormal <- function(xn){
  n = length(xn)
  y<-numeric(n)
  mu<-0
  for(i in 1:length(xn)){
    if (xn[i]>0){
      y[i] <- log(xn[i])
      mu <- mu + y[i]
    }
  }
  mu <- mu/n
  sigmasq <- (1/n)*sum((y-mu)**2)
  return (c(mu, sigmasq))
} 

param <- MLE_lognormal(new_X <- unname(unlist(pol[2])))

sample <- numeric(length = 52)
for (i in 1:52){
  sample[i] <- lognormal(df$retained....[i], param[1], param[2] ) }
hist(sample)
lines(df$startpoint, df$retained....)
```

```{r null param, echo=FALSE}
null_param <- function(param){
  res <-0
  for(i in 1:length(param)){
    if (param[i] <0.1){
      res <- res +1 
    }
  }
  return(res)
}
```



```{r optimization 3, echo=FALSE}
#p_init <- 0.5
#mu1_init <- 0.25
#mu2_init <- 0.75
#sigma1_init <- 0.25
#sigma2_init <- 0.5
new_X <- unname(unlist(pol[2]))


estimate_2 <- function(new_X, p_init, mu1_init, mu2_init, sigma1_init, sigma2_init ){
#criterion : epsilon 
eps = 0.01
k=0
err = 20000
old_likelihood <- 0 
new_likelihood <- 0 
N <- length(new_X)
p_new <- numeric(N)

param <- rep(1,5)

Y <- numeric(length = length(new_X))

for (i in 1:length(new_X)){
  if (new_X[i]>0){
    Y[i] <- log(new_X[i])}}

while(err>eps){
    if (null_param(param) == 0){
  #log likelihood computation
    for (i in 1:length(Y)){
      a <- bilognorm(new_X[i], mu1_init, mu2_init, sigma1_init, sigma2_init, p_init)
      if (is.na(a) == F){
        old_likelihood <- old_likelihood + a}} 
  #old_likelihood <- log(old_likelihood)
  
  #new parameters
    f_theta <- p_init*dnorm(Y, mean=mu1_init, sd = sigma1_init) + (1-p_init)*dnorm(Y, mean=mu2_init, sd = sigma2_init)
    
    if (test_0(f_theta) >0 ){
      gamma <- rep(0, n)
    } else{gamma <- p_init*dnorm(Y, mean=mu1_init, sd = sigma1_init)/f_theta}
    s <- sum(gamma)
    p_new <- s/N
    mu_1_new <- sum(Y*gamma, na.rm=T)/s
    mu_2_new <- sum(Y*(1-gamma), na.rm=T)/(N-s)
    s1 <- sum(gamma*((Y - mu1_init)**2), na.rm=T)
    s2 <- sum((1-gamma)*((Y - mu2_init)**2), na.rm=T)
    sigma1_new <- sqrt(s1/(N-s))
    sigma2_new <- sqrt(s2/(N-s))
    
    #new log likelihood computation
    for (i in 1:length(X)){
      a <- bilognorm(X[i], mu_1_new, mu_2_new, sigma1_new, sigma2_new, p_new)
      if (is.na(a) == F){
      new_likelihood <- new_likelihood + a}}
    #new_likelihood <- log(new_likelihood)
    
    #k represents the number of iterations before convergence
    k = k+1
    
    #err is the error 
    err = abs(new_likelihood - old_likelihood)
    
    #new parameters
    mu1_init <- mu_1_new
    mu2_init <- mu_2_new
    sigma1_init <- sigma1_new
    sigma2_init <- sigma2_new
    p_init <- p_new
    
    param <- c(mu1_init,mu2_init, sigma1_init,sigma2_init,p_init)
    
    #we put the likelihood at 0 to be able to compute them again 
    old_likelihood <- 0 
    new_likelihood <- 0 
    
    }else{err=0}
  return(param)
}}

new_param <- estimate_2(new_X, p_init, mu1_init, mu2_init, sigma1_init, sigma2_init)

print(new_param)
```

```{r kde plots, echo=FALSE}
par(mfrow=c(1,2))
sample <- numeric(length = 52)
for (i in 1:52){
  sample[i] <- bilognorm(df$retained....[i], new_param[2], new_param[3], new_param[4], new_param[5], new_param[1]) }
hist(sample, main='Fitted model with KDE')
lines(df$startpoint, df$retained....)

sample <- numeric(length = 52)
for (i in 1:52){
  sample[i] <- bilognorm(df$retained....[i], mu1, mu2, sigma1, sigma2, p) }
hist(sample, main='Fitted model with EM algorithm')
lines(df$startpoint, df$retained....)
```




## Bayesian approach 
```{r MCMC, echo=FALSE}
likelihood_mixed <- function(xn, mu1, mu2, sigma1, sigma2, p){
  old_likelihood <- 0
  for (i in 1:length(xn)){
    old_likelihood<- old_likelihood + bilognorm(xn[i], mu1, mu2, sigma1, sigma2, p)
  return(old_likelihood)}}


n_iter_max <- 11000
burn_in <- 1000 
thin <- 2

#initialize the parameters 
mu1 <- new_param[2]
mu2 <- new_param[3]
sigma1 <- new_param[4]
sigma2 <- new_param[5]
p <- new_param[1]
params <- c(mu1, mu2, sigma1, sigma2, p)

chain <- matrix(0, nrow = n_iter_max, ncol = length(params))
chain[1,] <- params

prop_sd <- c(0.01, 0.01, 0.01, 0.01, 0.01)

for (i in 2:n_iter_max) {
  # Generate proposal
  prop <- rnorm(length(params), mean = chain[i-1,], sd = prop_sd)
  prop <- abs(prop)
  
  # Compute acceptance probability
  #p_prop <- sum(dnorm(X, mean = c(prop[1], prop[2], prop[3], prop[4], prop[5]), sd=prop_sd))
  p_prop = 1
  p_curr = 1 # we have a gaussian proposal that is symmetric so the ratio of p_prop/p_curr is 1 (I set both at 1 to have the ratio equal to 1) 
  #p_curr <- sum(dnorm(X, mean = chain[i-1,], sd = prop_sd))
  accept_prob <- likelihood_mixed(X, prop[1], prop[2], prop[3], prop[4], prop[5])*p_prop/(likelihood_mixed(X, chain[i-1,][1], chain[i-1,][2], chain[i-1,][3], chain[i-1,][4], chain[i-1,][5])*p_curr)
  
  # Decide whether to accept or reject proposal
  if (runif(1) < accept_prob) {
    chain[i,] <- prop
  } else {
    chain[i,] <- chain[i-1,]
  }
}

posterior <- chain[-(1:burn_in),]
posterior <- posterior[seq(1, nrow(posterior), thin),]

```

Check convergence of the chain... bof bof 

```{r convergence, echo=FALSE}
for (i in 1:5) {
  plot(posterior[,i], type="l")
}
```

Check if it improved our model 

```{r improvement, echo=FALSE}
sample <- numeric(length = 52)
for (i in 1:52){
  sample[i] <- bilognorm(df$retained....[i], prop[1], prop[2], prop[3], prop[4], prop[5]) }
hist(sample)
lines(df$startpoint, df$retained....)
``` 

# Parametric bootstrap
We use again what we did during the first semester

We implement in this section a parametric bootstrap algorithm [] to assess the goodness of fit of our distribution. To do so, we sample from the fitted distribution with our estimated parameters. We use the Kolmogorov-Smirnov statistic $T = \sup_x \Big| \widehat{F}_N(x) - F_\widehat{\lambda}(x) \Big|$, where we suppose that under $H_0$ our estimated parameters $\widehat{\lambda}$ are consistent to the real parameters ${\lambda}_0$.

We then generate resamples and estimate the parameters on those new resamples. We are then able to compute the empirical distribution function and therefore compute the T statistic. We can then compute the p-value of the test.

```{r parametric bootstrap 2, echo=FALSE}
T_stat <- function(xn, mu1, mu2, sigma1, sigma2,p){
  F_N <- ecdf(xn) #the ecdf of our sample
  n = length(xn) #size of the sample
  evaluate_diff <- runif(n, min(xn), max(xn)) 
  sample <- rep(0,n)
  for (i in 1:n){
    sample[i] <- bilognorm(evaluate_diff[i], mu1, mu2, sigma1, sigma2, p)
  }
  F_star <- ecdf(sample)
  #difabs <- abs(F_N(grid) - plnormMix(grid, meanlog1 = abs(mu1), sdlog1 = abs(sigma1), meanlog2 = abs(mu2), sdlog2 = abs(sigma2), p.mix = p) )
  difabs <- abs(F_N(evaluate_diff) - F_star(evaluate_diff) )
  return(max(difabs))
}


#we use the parameters we estimated before
mu1_boot <- new_param[2]
mu2_boot <- new_param[3]
sigma1_boot <- new_param[4]
sigma2_boot <- new_param[5]
p_boot <- new_param[1]
T_0 <- T_stat(X, mu1_boot, mu2_boot, sigma1_boot, sigma2_boot, p_boot) #ok 

Bootstrap <- function(xn, T_0, mu1, mu2, sigma1, sigma2, p, B){
  n = length(xn)
  s <- 0
  #first we utilize our sample ie df$retained.... and estimate our parameters (with the EM algo, we expect the same parameters as previously)
  #param = estimate(xn, p, mu1, mu2, sigma1, sigma2)
  #mu1_estim <- param[1]
  #mu2_estim <- param[2]
  #sigma1_estim <- param[3]
  #sigma2_estim <- param[4]
  #p_estim <- param[5]
  #T_0 <- T_stat(xn, mu1_estim, mu2_estim, sigma1_estim, sigma2_estim, p_estim) 
  #We will then compare T_0 and the other values of T obtained with our bootstrap
  
  #Bootstrap
  T_boot <- rep(NA, B)
  x_resamples <- matrix(NA, nrow = B, ncol = n) #to put our resamples
  Y <- matrix(NA, nrow = B, ncol = n) 
  for (i in 1:B) {
    x_resamples[i,] <- sample(xn, replace = TRUE)
  }
  for(i in 1:B){
    #Y <- numeric(length = length(x_resamples[i,]))
    for (k in 1:length(x_resamples[i,])){
    if (x_resamples[i,][k]>0){
      Y[i] <- log(x_resamples[i,][k])}}

    param = estimate_2(x_resamples[i,], p, mu1, mu2, sigma1, sigma2)
    mu1_estim <- param[1]
    mu2_estim <- param[2]
    sigma1_estim <- param[3]
    sigma2_estim <- param[4]
    p_estim <- param[5]
    T_boot[i] <- T_stat(x_resamples[i,], mu1_estim, mu2_estim, sigma1_estim, sigma2_estim, p_estim) 
  }
  for (i in 1:length(T_boot)){
    if (T_boot[i]>T_0){
      s <- s+1
    }
  }
  return((1/(B+1))*(1 + s))
}

Bootstrap(X, T_0, mu1, mu2, sigma1, sigma2, p, 100)

a <- c(0, 1, 2, 3, 4)
boxplot(a, main='Pvalue')
```

A regarder car marche pour les premiers


```{r parametric bootstrap, echo=FALSE}
library("EnvStats")
abs_diff <- function(xn, mu1, mu2, sigma1, sigma2,p){
  #Compute EDF
  F_N <- ecdf(xn)
  #Compute absolute difference
  n=length(xn)
  grid<-runif(n, 0, 10)
  difabs <- abs(F_N(grid) - plnormMix(grid, meanlog1 = mu1, sdlog1 = sigma1, meanlog2 = mu2, sdlog2 = sigma2, p.mix = p))
  #Store result
  return(max(difabs))
}


Param_bootstrap <- function(xn, Bboot, mu1_init, mu2_init, sigma1_init, sigma2_init, p_init){
  #Compute parameters of sample xn
  n = length(xn)
  T0 <- abs_diff(xn, mu1_init, mu2_init, sigma1_init, sigma2_init, p_init)
  s <- 0 
  
  #Bootstrap procedure
  T_boot <- rep(NA,Bboot)
  for (b in 1:Bboot){
    #Generate sample from approximate candidate law 
    xb_star <- rlnormMix(n, mu1_init,sigma1_init, mu2_init, sigma2_init, p_init)
    #Estimate parameters of the sample
    estim <- estimate(xb_star, p_init, mu1_init,sigma1_init, mu2_init, sigma2_init) #change with MLE estimates ? 
    mu1_estim <- estim[1]
    mu2_estim <- estim[2]
    sigma1_estim <- estim[3]
    sigma2_estim <- estim[4]
    p_estim <- estim[5]

    #Difference
    T_boot[b]<-abs_diff(xb_star, mu1_estim, mu2_estim, sigma1_estim, sigma2_estim, p_estim )
    
  }
  for (i in 1:length(T_boot)){
    if (T_boot[i]>T0){
      s <- s+1
    }
  }
  return(1/(Bboot+1)*(1 + s))
}

Param_bootstrap(X, Bboot = 1000, mu1_init = mu1_init, mu2_init = mu2_init, sigma1_init = sigma1_init, sigma2_init = sigma2_init, p_init = p_init)

#pb in the algo
```
