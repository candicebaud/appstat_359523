---
title: "candice applied stat"
output: html_document
date: "2023-02-20"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r working directory and data, echo=FALSE, include=FALSE}
lapply(c("dplyr","chron","ggplot2","tidyr","questionr","survival","forcats","tidyselect",
         "data.table","table1","lubridate", "ggpubr","viridis","finalfit","survminer",
         "ggpubr", "ggthemes", "gridExtra", "rstatix","stringr",
         "wesanderson","kableExtra", "naniar","boot","scales","ggsci", "stringr",
         "Hmisc","DescTools","swimplot", 'stats', 'EnvStats'), 
       library, character.only=TRUE)

Sys.setlocale("LC_TIME", "English")
df <- read.csv("1_snow_particles.csv")
```

# Exploratory data analysis
I start by doing a simple exploratory data analysis by plotting the points. 
```{r exploratory, echo=FALSE}
plot(df$X,df$retained..../100)
df$binsize <- df$endpoint - df$startpoint

plot(log(df$retained..../100))

library('EnvStats')
hist(rlnormMix(52, meanlog1 = log(10), sdlog1 = 0.5, meanlog2 = log(35), sdlog2 = 1, p.mix = 0.2), freq = F )
lines(df$X,df$retained..../100)

```

```{r densities, echo=FALSE}
lognormal <- function(x, mu1, sigma1){
  if (x>0){
    return((1/x)*dnorm(log(x), mean=mu1, sd=sigma1))}
  else{
    return(0)
  }
}


bilognorm <- function(x, mu1, mu2, sigma1, sigma2, p) {
    return(p*lognormal(x,mu1,sigma1)+(1-p)*lognormal(x,mu2,sigma2))
}
```

# Fit a mixture of log normal distributions
## EM algorithm

```{r test_0, echo=FALSE}
test_0 <- function(xn){
  res <-0
  for (i in 1:length(xn)){
    if (xn[i] ==0){
      res <- res + 1
    }
  }
  return(res)}

```

```{r bi log normal EM algo, echo=FALSE}
#Initialisation 
p_init <- 0.5
mu1_init <- 0.25
mu2_init <- 0.75
sigma1_init <- 0.25
sigma2_init <- 0.5
X <- df$retained....

estimate <- function(X, p_init, mu1_init, mu2_init, sigma1_init, sigma2_init){


Y <- numeric(length = length(X))


for (i in 1:length(X)){
  if (X[i]!=0){
    Y[i] <- log(X[i])}}

N = length(X)

#criterion : epsilon 
eps = 0.001
k=0
err = 20000
old_likelihood <- 0 
new_likelihood <- 0 
p_new <- numeric(length=N)

while (err>eps){
  
#log likelihood computation
for (i in 1:length(X)){
  old_likelihood <- old_likelihood + bilognorm(X[i], mu1_init, mu2_init, sigma1_init, sigma2_init, p_init)} 
old_likelihood <- log(old_likelihood)

#new parameters
f_theta <- p_init*dnorm(Y, mean=mu1_init, sd = sigma1_init) + (1-p_init)*dnorm(Y, mean=mu2_init, sd = sigma2_init)

if (test_0(f_theta) >0 ){
   gamma <- rep(0, n)
}
else {gamma <- p_init*dnorm(Y, mean=mu1_init, sd = sigma1_init)/f_theta}

s <- sum(gamma)
p_new <- s/N
mu_1_new <- sum(Y*gamma)/s
mu_2_new <- sum(Y*(1-gamma))/(N-s)
s1 <- sum(gamma*(Y - mu1_init)**2)
s2 <- sum((1-gamma)*(Y - mu2_init)**2)
sigma1_new <- sqrt(s1/(N-s))
sigma2_new <- sqrt(s2/(N-s))

#new log likelihood computation
for (i in 1:length(X)){
  new_likelihood <- new_likelihood + bilognorm(X[i], mu_1_new, mu_2_new, sigma1_new, sigma2_new, p_new)}
new_likelihood <- log(new_likelihood)

#k represents the number of iterations before convergence
k = k+1

#err is the error 
err = abs(new_likelihood- old_likelihood)

#new parameters
mu1_init <- mu_1_new
mu2_init <- mu_2_new
sigma1_init <- sigma1_new
sigma2_init <- sigma2_new
p_init <- p_new

#we put the likelihood at 0 to be able to compute them again 
old_likelihood <- 0 
new_likelihood <- 0 


}

return(c(mu1_init, mu2_init, sigma1_init, sigma2_init, p_init))}

param <- estimate(X = X, p_init = p_init, mu1_init = mu1_init, mu2_init =mu2_init, sigma1_init = sigma1_init, sigma2_init = sigma2_init )

print(c('The obtained parameters are', param))
```

I plot below my data points and the the density I obtain with the estimated parameters.

```{r bi log normal fit, echo=FALSE}
mu1_init <- param[1]
mu2_init <- param[2]
sigma1_init <- param[3]
sigma2_init <- param[4]
p_init <- param[5]

sample <- numeric(length = 52)
for (i in 1:52){
  sample[i] <- bilognorm(df$retained....[i], mu1_init, mu2_init, sigma1_init, sigma2_init, p_init) }
hist(sample)
lines(df$startpoint, df$retained....)
#lines(df$retained...., bilognorm(df$retained...., mu1_init, mu2_init, sigma1_init, sigma2_init, p_init))
```

## Optimization
We want in this section to optimize our parameters estimation. 
```{r optimization, echo=FALSE}
#values we obtained and that we want to optimize 
mu1 <- mu1_init
mu2 <- mu2_init
sigma1 <- sigma1_init
sigma2 <- sigma2_init
p <- p_init

#we define eps from which we will make our values change to see which model is better fitting our data
eps = 0.1

par(mfrow = c(3,2))

#graph1
sample <- numeric(length = 52)
for (i in 1:52){
  sample[i] <- bilognorm(df$retained....[i], mu1_init, mu2_init, sigma1_init, sigma2_init, p_init) }
hist(sample, main='graph1')
lines(df$startpoint, df$retained....)

#graph2
sample <- numeric(length = 52)
for (i in 1:52){
  sample[i] <- bilognorm(df$retained....[i], mu1_init+eps, mu2_init+eps, sigma1_init+eps, sigma2_init+eps, p_init+2*eps) }
hist(sample, main='graph2')
lines(df$startpoint, df$retained....)

#graph3
sample <- numeric(length = 52)
for (i in 1:52){
  sample[i] <- bilognorm(df$retained....[i], mu1_init-eps, mu2_init-eps, sigma1_init-eps, sigma2_init-eps, p_init+2*eps) }
hist(sample, main='graph3')
lines(df$startpoint, df$retained....)

#graph4
sample <- numeric(length = 52)
for (i in 1:52){
  sample[i] <- bilognorm(df$retained....[i], mu1_init-eps, mu2_init+eps, sigma1_init+eps, sigma2_init+eps, p_init+2*eps) }
hist(sample, main='graph4')
lines(df$startpoint, df$retained....)

#graph5
sample <- numeric(length = 52)
for (i in 1:52){
  sample[i] <- bilognorm(df$retained....[i], mu1_init-eps, mu2_init+2*eps, sigma1_init, sigma2_init, p_init+2*eps) }
hist(sample, main='graph5')
lines(df$startpoint, df$retained....)

#graph6
sample <- numeric(length = 52)
for (i in 1:52){
  sample[i] <- bilognorm(df$retained....[i], mu1_init-eps, mu2_init+2*eps, sigma1_init + eps, sigma2_init, p_init+2*eps) }
hist(sample, main = 'graph6')
lines(df$startpoint, df$retained....)
```
```{r optimization 2, echo=FALSE}
library('KernSmooth')

plot(df$startpoint, df$retained...., type ='l', col = 'red')
pol <- locpoly(df$startpoint, df$retained...., drv = 0, degree =2, kernel = "normal", 
        bandwidth = 0.25 )
lines(pol)

```

If we try to estimate only a 'simple' lognormal.
```{r test optim, echo=FALSE}
MLE_lognormal <- function(xn){
  n = length(xn)
  y<-numeric(n)
  mu<-0
  for(i in 1:length(xn)){
    if (xn[i]>0){
      y[i] <- log(xn[i])
      mu <- mu + y[i]
    }
  }
  mu <- mu/n
  sigmasq <- (1/n)*sum((y-mu)**2)
  return (c(mu, sigmasq))
} 

param <- MLE_lognormal(new_X <- unname(unlist(pol[2])))

sample <- numeric(length = 52)
for (i in 1:52){
  sample[i] <- lognormal(df$retained....[i], param[1], param[2] ) }
hist(sample)
lines(df$startpoint, df$retained....)
```

```{r null param, echo=FALSE}
null_param <- function(param){
  res <-0
  for(i in 1:length(param)){
    if (param[i] <0.1){
      res <- res +1 
    }
  }
  return(res)
}
```



```{r optimization 3, echo=FALSE}
#p_init <- 0.5
#mu1_init <- 0.25
#mu2_init <- 0.75
#sigma1_init <- 0.25
#sigma2_init <- 0.5
new_X <- unname(unlist(pol[2]))


estimate_2 <- function(new_X, p_init, mu1_init, mu2_init, sigma1_init, sigma2_init ){
#criterion : epsilon 
eps = 0.01
k=0
err = 20000
old_likelihood <- 0 
new_likelihood <- 0 
N <- length(new_X)
p_new <- numeric(N)

param <- rep(1,5)

Y <- numeric(length = length(new_X))

for (i in 1:length(new_X)){
  if (new_X[i]>0){
    Y[i] <- log(new_X[i])}}

while(err>eps){
    if (null_param(param) == 0){
  #log likelihood computation
    for (i in 1:length(Y)){
      a <- bilognorm(new_X[i], mu1_init, mu2_init, sigma1_init, sigma2_init, p_init)
      if (is.na(a) == F){
        old_likelihood <- old_likelihood + a}} 
  #old_likelihood <- log(old_likelihood)
  
  #new parameters
    f_theta <- p_init*dnorm(Y, mean=mu1_init, sd = sigma1_init) + (1-p_init)*dnorm(Y, mean=mu2_init, sd = sigma2_init)
    
    if (test_0(f_theta) >0 ){
      gamma <- rep(0, n)
    } else{gamma <- p_init*dnorm(Y, mean=mu1_init, sd = sigma1_init)/f_theta}
    s <- sum(gamma)
    p_new <- s/N
    mu_1_new <- sum(Y*gamma, na.rm=T)/s
    mu_2_new <- sum(Y*(1-gamma), na.rm=T)/(N-s)
    s1 <- sum(gamma*((Y - mu1_init)**2), na.rm=T)
    s2 <- sum((1-gamma)*((Y - mu2_init)**2), na.rm=T)
    sigma1_new <- sqrt(s1/(N-s))
    sigma2_new <- sqrt(s2/(N-s))
    
    #new log likelihood computation
    for (i in 1:length(X)){
      a <- bilognorm(X[i], mu_1_new, mu_2_new, sigma1_new, sigma2_new, p_new)
      if (is.na(a) == F){
      new_likelihood <- new_likelihood + a}}
    #new_likelihood <- log(new_likelihood)
    
    #k represents the number of iterations before convergence
    k = k+1
    
    #err is the error 
    err = abs(new_likelihood - old_likelihood)
    
    #new parameters
    mu1_init <- mu_1_new
    mu2_init <- mu_2_new
    sigma1_init <- sigma1_new
    sigma2_init <- sigma2_new
    p_init <- p_new
    
    param <- c(mu1_init,mu2_init, sigma1_init,sigma2_init,p_init)
    
    #we put the likelihood at 0 to be able to compute them again 
    old_likelihood <- 0 
    new_likelihood <- 0 
    
    }else{err=0}
  return(param)
}}

new_param <- estimate_2(new_X, p_init, mu1_init, mu2_init, sigma1_init, sigma2_init)

print(new_param)
```

```{r kde plots, echo=FALSE}
par(mfrow=c(1,2))
sample <- numeric(length = 52)
for (i in 1:52){
  sample[i] <- bilognorm(df$retained....[i], new_param[2], new_param[3], new_param[4], new_param[5], new_param[1]) }
hist(sample, main='Fitted model with KDE')
lines(df$startpoint, df$retained....)

sample <- numeric(length = 52)
for (i in 1:52){
  sample[i] <- bilognorm(df$retained....[i], mu1, mu2, sigma1, sigma2, p) }
hist(sample, main='Fitted model with EM algorithm')
lines(df$startpoint, df$retained....)
```




## Bayesian approach 
```{r MCMC, echo=FALSE}
likelihood_mixed <- function(xn, mu1, mu2, sigma1, sigma2, p){
  old_likelihood <- 0
  for (i in 1:length(xn)){
    old_likelihood<- old_likelihood + bilognorm(xn[i], mu1, mu2, sigma1, sigma2, p)
  return(old_likelihood)}}


n_iter_max <- 11000
burn_in <- 1000 
thin <- 2

#initialize the parameters 
mu1 <- new_param[2]
mu2 <- new_param[3]
sigma1 <- new_param[4]
sigma2 <- new_param[5]
p <- new_param[1]
params <- c(mu1, mu2, sigma1, sigma2, p)

chain <- matrix(0, nrow = n_iter_max, ncol = length(params))
chain[1,] <- params

prop_sd <- c(0.01, 0.01, 0.01, 0.01, 0.01)

for (i in 2:n_iter_max) {
  # Generate proposal
  prop <- rnorm(length(params), mean = chain[i-1,], sd = prop_sd)
  prop <- abs(prop)
  
  # Compute acceptance probability
  #p_prop <- sum(dnorm(X, mean = c(prop[1], prop[2], prop[3], prop[4], prop[5]), sd=prop_sd))
  p_prop = 1
  p_curr = 1 # we have a gaussian proposal that is symmetric so the ratio of p_prop/p_curr is 1 (I set both at 1 to have the ratio equal to 1) 
  #p_curr <- sum(dnorm(X, mean = chain[i-1,], sd = prop_sd))
  accept_prob <- likelihood_mixed(X, prop[1], prop[2], prop[3], prop[4], prop[5])*p_prop/(likelihood_mixed(X, chain[i-1,][1], chain[i-1,][2], chain[i-1,][3], chain[i-1,][4], chain[i-1,][5])*p_curr)
  
  # Decide whether to accept or reject proposal
  if (runif(1) < accept_prob) {
    chain[i,] <- prop
  } else {
    chain[i,] <- chain[i-1,]
  }
}

posterior <- chain[-(1:burn_in),]
posterior <- posterior[seq(1, nrow(posterior), thin),]

```

Check convergence of the chain... bof bof 

```{r convergence, echo=FALSE}
for (i in 1:5) {
  plot(posterior[,i], type="l")
}
```

Check if it improved our model 

```{r improvement, echo=FALSE}
sample <- numeric(length = 52)
for (i in 1:52){
  sample[i] <- bilognorm(df$retained....[i], prop[1], prop[2], prop[3], prop[4], prop[5]) }
hist(sample)
lines(df$startpoint, df$retained....)
``` 

# Parametric bootstrap
We use again what we did during the first semester

We implement in this section a parametric bootstrap algorithm [] to assess the goodness of fit of our distribution. To do so, we sample from the fitted distribution with our estimated parameters. We use the Kolmogorov-Smirnov statistic $T = \sup_x \Big| \widehat{F}_N(x) - F_\widehat{\lambda}(x) \Big|$, where we suppose that under $H_0$ our estimated parameters $\widehat{\lambda}$ are consistent to the real parameters ${\lambda}_0$.

We then generate resamples and estimate the parameters on those new resamples. We are then able to compute the empirical distribution function and therefore compute the T statistic. We can then compute the p-value of the test.

```{r parametric bootstrap 2, echo=FALSE}
T_stat <- function(xn, mu1, mu2, sigma1, sigma2,p){
  F_N <- ecdf(xn) #the ecdf of our sample
  n = length(xn) #size of the sample
  evaluate_diff <- runif(n, min(xn), max(xn)) 
  sample <- rep(0,n)
  for (i in 1:n){
    sample[i] <- bilognorm(evaluate_diff[i], mu1, mu2, sigma1, sigma2, p)
  }
  F_star <- ecdf(sample)
  #difabs <- abs(F_N(grid) - plnormMix(grid, meanlog1 = abs(mu1), sdlog1 = abs(sigma1), meanlog2 = abs(mu2), sdlog2 = abs(sigma2), p.mix = p) )
  difabs <- abs(F_N(evaluate_diff) - F_star(evaluate_diff) )
  return(max(difabs))
}


#we use the parameters we estimated before
mu1_boot <- new_param[2]
mu2_boot <- new_param[3]
sigma1_boot <- new_param[4]
sigma2_boot <- new_param[5]
p_boot <- new_param[1]
T_0 <- T_stat(X, mu1_boot, mu2_boot, sigma1_boot, sigma2_boot, p_boot) #ok 

Bootstrap <- function(xn, T_0, mu1, mu2, sigma1, sigma2, p, B){
  n = length(xn)
  s <- 0
  #first we utilize our sample ie df$retained.... and estimate our parameters (with the EM algo, we expect the same parameters as previously)
  #param = estimate(xn, p, mu1, mu2, sigma1, sigma2)
  #mu1_estim <- param[1]
  #mu2_estim <- param[2]
  #sigma1_estim <- param[3]
  #sigma2_estim <- param[4]
  #p_estim <- param[5]
  #T_0 <- T_stat(xn, mu1_estim, mu2_estim, sigma1_estim, sigma2_estim, p_estim) 
  #We will then compare T_0 and the other values of T obtained with our bootstrap
  
  #Bootstrap
  T_boot <- rep(NA, B)
  x_resamples <- matrix(NA, nrow = B, ncol = n) #to put our resamples
  Y <- matrix(NA, nrow = B, ncol = n) 
  for (i in 1:B) {
    x_resamples[i,] <- sample(xn, replace = TRUE)
  }
  for(i in 1:B){
    #Y <- numeric(length = length(x_resamples[i,]))
    for (k in 1:length(x_resamples[i,])){
    if (x_resamples[i,][k]>0){
      Y[i] <- log(x_resamples[i,][k])}}

    param = estimate_2(x_resamples[i,], p, mu1, mu2, sigma1, sigma2)
    mu1_estim <- param[1]
    mu2_estim <- param[2]
    sigma1_estim <- param[3]
    sigma2_estim <- param[4]
    p_estim <- param[5]
    T_boot[i] <- T_stat(x_resamples[i,], mu1_estim, mu2_estim, sigma1_estim, sigma2_estim, p_estim) 
  }
  for (i in 1:length(T_boot)){
    if (T_boot[i]>T_0){
      s <- s+1
    }
  }
  return((1/(B+1))*(1 + s))
}

Bootstrap(X, T_0, mu1, mu2, sigma1, sigma2, p, 100)

a <- c(0, 1, 2, 3, 4)
boxplot(a, main='Pvalue')
```

A regarder car marche pour les premiers


```{r parametric bootstrap, echo=FALSE}
library("EnvStats")
abs_diff <- function(xn, mu1, mu2, sigma1, sigma2,p){
  #Compute EDF
  F_N <- ecdf(xn)
  #Compute absolute difference
  n=length(xn)
  grid<-runif(n, 0, 10)
  difabs <- abs(F_N(grid) - plnormMix(grid, meanlog1 = mu1, sdlog1 = sigma1, meanlog2 = mu2, sdlog2 = sigma2, p.mix = p))
  #Store result
  return(max(difabs))
}


Param_bootstrap <- function(xn, Bboot, mu1_init, mu2_init, sigma1_init, sigma2_init, p_init){
  #Compute parameters of sample xn
  n = length(xn)
  T0 <- abs_diff(xn, mu1_init, mu2_init, sigma1_init, sigma2_init, p_init)
  s <- 0 
  
  #Bootstrap procedure
  T_boot <- rep(NA,Bboot)
  for (b in 1:Bboot){
    #Generate sample from approximate candidate law 
    xb_star <- rlnormMix(n, mu1_init,sigma1_init, mu2_init, sigma2_init, p_init)
    #Estimate parameters of the sample
    estim <- estimate(xb_star, p_init, mu1_init,sigma1_init, mu2_init, sigma2_init) #change with MLE estimates ? 
    mu1_estim <- estim[1]
    mu2_estim <- estim[2]
    sigma1_estim <- estim[3]
    sigma2_estim <- estim[4]
    p_estim <- estim[5]

    #Difference
    T_boot[b]<-abs_diff(xb_star, mu1_estim, mu2_estim, sigma1_estim, sigma2_estim, p_estim )
    
  }
  for (i in 1:length(T_boot)){
    if (T_boot[i]>T0){
      s <- s+1
    }
  }
  return(1/(Bboot+1)*(1 + s))
}

Param_bootstrap(X, Bboot = 1000, mu1_init = mu1_init, mu2_init = mu2_init, sigma1_init = sigma1_init, sigma2_init = sigma2_init, p_init = p_init)

#pb in the algo
```


### MCMC 
### Bayesian approach
In this section, I use Markov Chain Monte Carlo technique (MCMC) to further refine the parameters obtained with local search. The idea behind MCMC is to construct a Markov chain whose stationary distribution is the distribution of interest. This is achieved by iteratively proposing new parameter values and accepting or rejecting them based on the likelihood of the data, given the proposed values. The result is a sequence of parameter values that, after a certain number of iterations, approximate the posterior distribution of the parameters.

To implement MCMC, I use a proposal distribution that generates new parameter values from the current value. In my case, I use a Gaussian proposal distribution with a standard deviation of 0.01 and an initial mean given by the parameters obtained in the previous section. The advantage of this proposal is that it is symmetric, which simplifies the acceptance/rejection step of the MCMC algorithm. 

```{r, MCMC, echo=FALSE}
likelihood_mixed <- function(xn, mu1, mu2, sigma1, sigma2, p){
  old_likelihood <- 0
  for (i in 1:length(xn)){
    old_likelihood<- old_likelihood + bilognorm(xn[i], mu1, mu2, sigma1, sigma2, p)
  return(old_likelihood)}}


n_iter_max <- 6000
burn_in <- 1000 
thin <- 2

prob_stock <- rep(0, n_iter_max)

#initialize the parameters 
mu1 <- new_param[2]
mu2 <- new_param[3]
sigma1 <- new_param[4]
sigma2 <- new_param[5]
p <- new_param[1]
params <- c(mu1, mu2, sigma1, sigma2, p)

chain <- matrix(0, nrow = n_iter_max, ncol = length(params))
chain[1,] <- params

prop_sd <- c(0.01, 0.01, 0.01, 0.01, 0.01)

for (i in 2:n_iter_max) {
  # Generate proposal
  prop <- rnorm(length(params), mean = chain[i-1,], sd = prop_sd)
  prop <- abs(prop)
  
  # Compute acceptance probability
  #p_prop <- sum(dnorm(X, mean = c(prop[1], prop[2], prop[3], prop[4], prop[5]), sd=prop_sd))
  p_prop = 1
  p_curr = 1 # I have a gaussian proposal that is symmetric so the ratio of p_prop/p_curr is 1 (I set both at 1 to have the ratio equal to 1) 
  #p_curr <- sum(dnorm(X, mean = chain[i-1,], sd = prop_sd))
  
  accept_prob <- likelihood_mixed(new_X, prop[1], prop[2], prop[3], prop[4], prop[5])*p_prop/(likelihood_mixed(new_X, chain[i-1,][1], chain[i-1,][2], chain[i-1,][3], chain[i-1,][4], chain[i-1,][5])*p_curr)
  if(accept_prob>=1){
    accept_prob <- 1
  }
  prob_stock[i] <- accept_prob
  
  # Decide whether to accept or reject proposal
  if (runif(1) < accept_prob) {
    chain[i,] <- prop
  } else {
    chain[i,] <- chain[i-1,]
  }
}

posterior <- chain[-(1:burn_in),]
posterior <- posterior[seq(1, nrow(posterior), thin),]

#new parameters
#mu1<- posterior[,1][length(posterior[,1])]
#mu2 <- posterior[,2][length(posterior[,2])]
#sigma1 <- posterior[,3][length(posterior[,3])]
#sigma2 <- posterior[,4][length(posterior[,4])]
#p <- posterior[,5][length(posterior[,5])]

```


*Figure5* allows us to see the new fitted distribution which doesn't appear better than the previous one. The generated histogram doesn't represent the true distribution accurately at all since the peaks are not accurately represented. I will discuss those results deeper in the following section.

```{r, fig.align="center", fig.cap="Figure 5: MCMC results"}
sample <- numeric(length = 52)
for (i in 1:52){
  sample[i] <- bilognorm(df$retained....[i], prop[1], prop[2], prop[3], prop[4], prop[5]) }
hist(sample, main='Fitted model with a MCMC method', xlab = 'sample values', ylab ='number of occurences', freq=F)
lines(df$startpoint, df$retained..../area, col='red')


#to_plot_x <- seq(0, 3, by=0.01)
#to_plot_y <- numeric(length(to_plot_x))
#for (i in 1:length(to_plot_x)){
  #to_plot_y[i]<- bilognorm(to_plot_x[i],mu1, mu2, sigma1, sigma2, p) }
#plot(df$startpoint,df$retained....)
#lines(to_plot_x, to_plot_y, type = 'l')
#plot(to_plot_x,to_plot_y*10)
#lines(df$startpoint,df$retained....)
``` 

### Convergence of the MCMC 

In *Figure7*, the first five plots show the values of the parameters after removing the burn-in period (the first 1000 values in this case). If the MCMC algorithm is working well, one would expect to see convergence of the parameter values over time, indicating that the chain has explored the parameter space and settled into a stable distribution. However, for all of the parameters, one does not see clear convergence over the course of the chain.

The last plot in *Figure7* shows the acceptance probability of the proposals, with the mean value shown in red. Ideally, the acceptance probability should be in the range of 10% to 50%, indicating that the proposals are exploring the space effectively. However, in that case, the acceptance probability is quite high, indicating that the chain is not exploring the space efficiently.

These issues with the MCMC procedure likely explains why the parameter estimates do not appear to improve in *Figure6*. I would need to investigate further to determine the cause of these problems and identify strategies for improving the performance of the MCMC algorithm.


```{r fig.align="center", fig.cap="Figure 7: MCMC convergence"}
par(mfrow = c(2,3))
plot(posterior[,1], type="l", main ='Values of mu1', xlab = 'Iterations', ylab ='mu1')
plot(posterior[,2], type="l", main ='Values of mu2', xlab = 'Iterations', ylab ='mu2')
plot(posterior[,3], type="l", main ='Values of sigma1', xlab = 'Iterations', ylab ='sigma1')
plot(posterior[,4], type="l", main ='Values of sigma2', xlab = 'Iterations', ylab ='sigma2')
plot(posterior[,5], type="l", main ='Values of p', xlab = 'Iterations', ylab ='p')
plot(prob_stock, type="l", main ='Acceptance probability', xlab = 'Iterations', ylab ='Acceptance probability')
abline(h = mean(prob_stock), col='red')
```


### KErnel 
In this section, I do some kernel density estimation. Kernel density estimation is a non-parametric technique used to estimate the probability density function of a random variable based on a sample of data. The idea behind kernel density estimation is to place a "kernel" function at each data point, and then sum the kernel functions to create a smooth estimate of the density function.
I thus estimate the density function as follows

$f(x) = \frac{1}{n h} \sum_{i=1}^n K\left(\frac{x - X_i}{h}\right)$

where h is a bandwidth parameter that controls the width of the kernel function and determines the degree of smoothing. A smaller bandwidth results in a more variable estimate, while a larger bandwidth results in a smoother estimate that may over-smooth the data.

Kernel functions are typically symmetric, non-negative functions with zero mean, such as the Gaussian kernel that I use here and that has the following density

$$f(x) = \frac{1}{\sigma \sqrt{2 \pi}} e^{-\frac{(x-\mu)^2}{2\sigma^2}}$$ 


*Figure3* represents the data distribution and the one estimated with the kernel density estimation.

```{r fig.align="center", fig.cap="Figure 3: Kernel density estimation"}
suppressWarnings(library('KernSmooth'))

plot(df$startpoint, df$retained...., type ='l', col = 'red', xlab = 'Bin startpoint', ylab = 'Proportion of particles(%)', main = 'Kernel density estimation and real data')
pol <- locpoly(df$startpoint, df$retained...., drv = 0, degree =2, kernel = "normal", 
        bandwidth = 0.25 )
lines(pol)

```


After re-evaluating the parameters, I refit the model to see if there were any improvements. *Figure4* shows the results with an histogram. There is a real improvement in the estimation thanks to the use of kernel density estimation. Specifically, one can see that the right peak is better estimated, suggesting that our modeling approach is capturing more of the underlying distribution. However, I note that the left peak is still over-estimated, which indicates that further optimization may be necessary to obtain an accurate model. 

```{r optimization 3, echo=FALSE}
#new_X is the kernel density estimation density
new_X <- unname(unlist(pol[2]))

new_param <- estimate(new_X, p_init, mu1_init, mu2_init, sigma1_init, sigma2_init)

print(c('The obtained values are', new_param))
```



```{r, fig.align="center", fig.cap="Figure 4: Kernel density estimation results"}
par(mfrow=c(1,2))

sample_mixture <- function(mu1, mu2, sigma1, sigma2, p){
  t<-rbern(1, p)
}


sample <- numeric(length = 52)
for (i in 1:52){
  sample[i] <- bilognorm(df$retained....[i], new_param[2], new_param[3], new_param[4], new_param[5], new_param[1]) }
hist(sample, main='Fitted model with KDE', freq=F, breaks = df$startpoint) #in %
lines(df$startpoint, df$retained...., col='red')

#df%>%group_by(startpoint, retained....)%>% ggplot(aes(x=startpoint, y=retained....)) + geom_bar(stat='identity')

sample <- numeric(length = 52)
for (i in 1:52){
  sample[i] <- bilognorm(df$retained....[i], mu1, mu2, sigma1, sigma2, p) }
hist(sample, main='Fitted model with EM algorithm', freq=F, breaks = df$startpoint)
lines(df$startpoint, df$retained...., col='red')

```



